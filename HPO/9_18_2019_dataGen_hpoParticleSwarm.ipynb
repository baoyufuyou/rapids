{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width = 85% src='rapids_motivation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width = 75% src='choices.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algos, datasets\n",
    "import xgboost; from xgboost import plot_tree\n",
    "from sklearn import datasets; from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rapids_lib_v9 as rl\n",
    "''' NOTE: anytime changes are made to rapids_lib.py you can either:\n",
    "      1. refresh/reload via the code below, OR\n",
    "      2. restart the kernel '''\n",
    "import importlib; importlib.reload(rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate data shapes [coordinate lists] and hand them to the GPU. The GPU will randomly build 3D blobs [ cupy.random.normal ] around each coordinate point to create a much larger, noisier, and more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coilPoints = rl.gen_coil( nPoints = 20, coilDensity = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure( figsize = ( 10, 5 )).add_subplot(111, projection='3d')\n",
    "ax.plot( coilPoints[:,0], coilPoints[:,1], coilPoints[:,2], 'purple')\n",
    "ax.scatter( coilPoints[:,0], coilPoints[:,1], coilPoints[:,2], color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this concept we offer the following dataset variations:\n",
    "1. Helix - two entwined coils, inspired by DNA casing\n",
    "2. Whirl - an increasingly unwinding Helix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_dataset_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helix and Whirl Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBlobPoints = 1000\n",
    "nCoordinates = 500\n",
    "sdevScales = [ .01, .01, .01]\n",
    "noiseScale = 1/10.\n",
    "coilDensity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, t_gen = rl.gen_blob_coils( coilType='whirl', shuffleFlag = False, \n",
    "                                         nBlobPoints = nBlobPoints,  \n",
    "                                         nCoordinates = nCoordinates, \n",
    "                                         sdevScales = sdevScales, \n",
    "                                         noiseScale = noiseScale, \n",
    "                                         coilDensity = coilDensity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, t_gen = rl.gen_blob_coils( coilType='helix', shuffleFlag = False, \n",
    "                                         nBlobPoints = nBlobPoints,  \n",
    "                                         nCoordinates = nCoordinates, \n",
    "                                         sdevScales = sdevScales, \n",
    "                                         noiseScale = noiseScale, \n",
    "                                         coilDensity = coilDensity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_nfolds ( dataDF, labelsDF, nFolds = 10, seed = 1, nSamplesToSwap = 50 ):\n",
    "    print('splitting data into training and test set')\n",
    "    startTime = time.time()\n",
    "    \n",
    "    nSamplesPerFold = int(dataDF.shape[0] // nFolds)\n",
    "    sampleRanges = np.arange(nFolds) * nSamplesPerFold\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    foldStartInds = np.random.randint(0, nFolds-1, size = nFolds)\n",
    "    foldEndInds = foldStartInds + 1 \n",
    "    \n",
    "    testFold = np.random.randint(0,nFolds-1)\n",
    "    trainInds = None; testInds = None\n",
    "    \n",
    "    for iFold in range( nFolds ):\n",
    "        lastFoldFlag = ( iFold == nFolds-1 )\n",
    "        if lastFoldFlag: foldInds = np.arange(sampleRanges[iFold], dataDF.shape[0] )\n",
    "        else: foldInds = np.arange(sampleRanges[iFold], sampleRanges[iFold+1])\n",
    "        \n",
    "        if iFold == testFold: testInds = foldInds\n",
    "        else:\n",
    "            if trainInds is None: trainInds = foldInds\n",
    "            else: trainInds = np.concatenate([trainInds, foldInds])\n",
    "                \n",
    "    # swap subset of train and test samples [ low values require higher model generalization ]\n",
    "    if nSamplesToSwap > 0:\n",
    "        trainIndsToSwap = np.random.permutation(trainInds.shape[0])[0:nSamplesToSwap]\n",
    "        testIndsToSwap = np.random.permutation(testInds.shape[0])[0:nSamplesToSwap]        \n",
    "        trainBuffer = trainInds[trainIndsToSwap].copy()\n",
    "        trainInds[trainIndsToSwap] = testInds[testIndsToSwap]\n",
    "        testInds[testIndsToSwap] = trainBuffer\n",
    "    \n",
    "    # build final dataframes\n",
    "    trainDF = dataDF.iloc[trainInds]\n",
    "    testDF = dataDF.iloc[testInds]\n",
    "    trainLabelsDF = labelsDF.iloc[trainInds]\n",
    "    testLabelsDF = labelsDF.iloc[testInds]                \n",
    "    \n",
    "    return trainDF, trainLabelsDF, testDF, testLabelsDF, time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rescale/Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe_inplace ( targetDF, trainMeans = {}, trainSTDevs = {} ):    \n",
    "    print('rescaling data')\n",
    "    sT = time.time()\n",
    "    for iCol in targetDF.columns:\n",
    "        \n",
    "        # omit scaling label column\n",
    "        if iCol == targetDF.columns[-1] == 'label': continue\n",
    "            \n",
    "        # compute means and standard deviations for each column [ should skip for test data ]\n",
    "        if iCol not in trainMeans.keys() and iCol not in trainSTDevs.keys():            \n",
    "            trainMeans[iCol] = targetDF[iCol].mean()\n",
    "            trainSTDevs[iCol] = targetDF[iCol].std()\n",
    "            \n",
    "        # apply scaling to each column\n",
    "        targetDF[iCol] = ( targetDF[iCol] - trainMeans[iCol] ) / ( trainSTDevs[iCol] + 1e-10 )\n",
    "        \n",
    "    return trainMeans, trainSTDevs, time.time() - sT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CPU** split & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "trainData_pDF, trainLabels_pDF, testData_pDF, testLabels_pDF, t_split_CPU = split_train_test_nfolds ( data.to_pandas(), labels.to_pandas(), nSamplesToSwap = 400 )\n",
    "\n",
    "# apply standard scaling\n",
    "trainMeans_CPU, trainSTDevs_CPU, t_scaleTrain_CPU = scale_dataframe_inplace ( trainData_pDF )\n",
    "_,_, t_scaleTest_CPU = scale_dataframe_inplace ( testData_pDF, trainMeans_CPU, trainSTDevs_CPU )    \n",
    "\n",
    "expLog = rl.update_log( expLog, [['CPU_split_train_test', t_split_CPU],\n",
    "                                 ['CPU_scale_train_data', t_scaleTrain_CPU], \n",
    "                                 ['CPU_scale_test_data', t_scaleTest_CPU]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPU** split & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF, t_split = split_train_test_nfolds ( data, labels, nSamplesToSwap = 400)\n",
    "\n",
    "# apply standard scaling\n",
    "trainMeans, trainSTDevs, t_scaleTrain = scale_dataframe_inplace ( trainData_cDF )\n",
    "_,_, t_scaleTest = scale_dataframe_inplace ( testData_cDF, trainMeans, trainSTDevs )    \n",
    "\n",
    "expLog = rl.update_log( expLog, [['GPU_split_train_test', t_split],\n",
    "                                 ['GPU_scale_train_data', t_scaleTrain],\n",
    "                                 ['GPU_scale_test_data', t_scaleTest]] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.plot_train_test(trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating a Single Model on CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' -------------------------------------------------------------------------\n",
    ">  GPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_GPU (trainData_cDF, testData_cDF, paramsGPU = {}):    \n",
    "    print('training xgboost model on GPU');  \n",
    "    startTime = time.time()    \n",
    "    \n",
    "    trainDMatrix = xgboost.DMatrix( trainData_cDF.to_pandas(), label = trainLabels_cDF.to_pandas())    \n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, params = paramsGPU, num_boost_round = paramsGPU['num_boost_rounds'] )\n",
    "    \n",
    "    return trainedModelGPU, time.time() - startTime\n",
    "\n",
    "def test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF ):\n",
    "    print('testing xgboost model on GPU')\n",
    "    startTime = time.time()   \n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( data = testData_cDF.to_pandas(), label = testLabels_cDF.to_pandas())    \n",
    "    predictionsGPU = trainedModelGPU.predict(testDMatrix)\n",
    "    \n",
    "    return predictionsGPU, time.time() - startTime\n",
    "\n",
    "''' -------------------------------------------------------------------------\n",
    ">  CPU Train and Test\n",
    "------------------------------------------------------------------------- '''\n",
    "def train_model_CPU (trainData_cDF, trainLabels_cDF, paramsCPU = {}):    \n",
    "    print('training xgboost model on {} CPU cores'.format(nCores) )\n",
    "\n",
    "    startTime = time.time()\n",
    "    \n",
    "    trainDMatrix = xgboost.DMatrix( trainData_cDF.to_pandas(), label = trainLabels_cDF.to_pandas())\n",
    "    \n",
    "    xgBoostModelCPU = xgboost.train( dtrain = trainDMatrix, params = paramsCPU, num_boost_round = paramsCPU['num_boost_rounds'])\n",
    "    \n",
    "    return xgBoostModelCPU, time.time() - startTime\n",
    "\n",
    "def test_model_CPU ( trainedModelCPU, testData_cDF, testLabels_cDF ):\n",
    "    print('testing xgboost model on CPU')\n",
    "    startTime = time.time()\n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( testData_cDF.to_pandas(), label = testLabels_cDF.to_pandas())\n",
    "    predictionsCPU = trainedModelCPU.predict(testDMatrix)\n",
    "    \n",
    "    return predictionsCPU, time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0])\n",
    "\n",
    "paramsCPU = {\n",
    "    'max_depth': 6,\n",
    "    'num_boost_rounds': 100,    \n",
    "    'learning_rate': .1,\n",
    "    'lambda': 1,    \n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'hist',\n",
    "    'n_jobs': nCores,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': 6,\n",
    "    'num_boost_rounds': 100,\n",
    "    'learning_rate': .1,\n",
    "    'lambda': 1,    \n",
    "    'objective': 'binary:hinge',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'n_gpus': 1,    \n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CPU** Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModelCPU, t_trainCPU = train_model_CPU ( trainData_cDF, trainLabels_cDF, paramsCPU )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsCPU, t_inferCPU = test_model_CPU ( trainedModelCPU, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = rl.update_log( expLog, [['CPU_model_training', t_trainCPU], ['CPU_model_inference', t_inferCPU]] ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPU** Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModelGPU, t_trainGPU = train_model_GPU ( trainData_cDF, trainLabels_cDF, paramsGPU )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsGPU, t_inferGPU = test_model_GPU ( trainedModelGPU, testData_cDF, testLabels_cDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog = rl.update_log( expLog, [['GPU_model_training', t_trainGPU], ['GPU_model_inference', t_inferGPU]] ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CPU and GPU Accuracy [ plot on train and test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels_pDF = testLabels_cDF.to_pandas()\n",
    "accuracyCPU = accuracy_score(testLabels_pDF, predictionsCPU)\n",
    "accuracyGPU = accuracy_score(testLabels_pDF, predictionsGPU)\n",
    "print( 'CPU accuracy = {}'.format( accuracyCPU ) )\n",
    "print( 'GPU accuracy = {}'.format( accuracyGPU ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMat = confusion_matrix(testLabels_pDF, predictionsGPU)\n",
    "print(cMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(100,50))\n",
    "plot_tree(trainedModelGPU, num_trees=0, ax=plt.subplot(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Search          \n",
    "> TODO: ADD early stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost;\n",
    "print( xgboost.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of workers [ changes require kernel restart ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(ip=\"\", n_workers = 4 )\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-scatter Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client is not None:        \n",
    "    scatteredData_future = client.scatter( [ trainData_cDF, testData_cDF ], broadcast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatteredData_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( '{} \\n {} \\n'.format( scatteredData_future[0].key, scatteredData_future[1].key ) )\n",
    "client.who_has(scatteredData_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Strategy - Particle Swarm [ Explore + Exploit ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_particles( paramRanges, particlesInTimestep, velocitiesInTimestep, bestParamsIndex, sBest = .75, sExplore = .25 , deltaTime = 1):\n",
    "    \n",
    "    nParticles = particlesInTimestep.shape[ 0 ]\n",
    "    nParameters = particlesInTimestep.shape[ 1 ]\n",
    "    best = particlesInTimestep[bestParamsIndex, :]\n",
    "    \n",
    "    bestRep = numpy.matlib.repmat( np.array( best ).reshape( -1, 1 ), nParticles, 1).reshape( nParticles, nParameters )\n",
    "    \n",
    "    # momentum + move to best + explore\n",
    "    velocitiesInTimestep += sBest * ( bestRep - particlesInTimestep ) \\\n",
    "                            + sExplore * ( np.random.randn( nParticles, nParameters ) )\n",
    "    \n",
    "    particlesInTimestep += velocitiesInTimestep * deltaTime \n",
    "    \n",
    "    # TODO: avoid duplicates\n",
    "    \n",
    "    # enforce param bounds\n",
    "    for iParam in range( nParameters ):\n",
    "        particlesInTimestep[ :, iParam ] = np.clip(particlesInTimestep[ :, iParam ], paramRanges[iParam][1], paramRanges[iParam][2])\n",
    "        if paramRanges[iParam][3] == 'int':\n",
    "            particlesInTimestep[ :, iParam ] = np.round( particlesInTimestep[ :, iParam ] )\n",
    "            \n",
    "    return particlesInTimestep, velocitiesInTimestep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Harness\n",
    "> Particle Evals [ Train and Test Logic ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_hpo ( trainData_cDF, trainLabels_cDF, particleParams, iParticle, iTimestep ):\n",
    "    \n",
    "    # fixed parameters\n",
    "    paramsGPU = { 'objective': 'binary:hinge',\n",
    "                  'tree_method': 'gpu_hist',\n",
    "                  'n_gpus': 1,\n",
    "                  'random_state': 0 }\n",
    "    \n",
    "    # parameters to search over\n",
    "    paramsGPU['max_depth'] = int(particleParams[0])\n",
    "    paramsGPU['learning_rate'] = particleParams[1]\n",
    "    paramsGPU['lambda'] = particleParams[2] \n",
    "    paramsGPU['num_boost_rounds'] = 1000\n",
    "    \n",
    "    startTime = time.time()\n",
    "    trainDMatrix = xgboost.DMatrix( data = trainData_cDF, label = trainLabels_cDF )\n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(trainDMatrix, 'train')], \n",
    "                                     params = paramsGPU,\n",
    "                                     num_boost_round = paramsGPU['num_boost_rounds'],\n",
    "                                     early_stopping_rounds = 15,\n",
    "                                     verbose_eval = False )\n",
    "    \n",
    "    elapsedTime = time.time() - startTime\n",
    "    print('training xgboost model on GPU t: {}, nP: {}, params [  {} {} {} ], time: {} '.format( iTimestep, iParticle, particleParams[0], particleParams[1], particleParams[2], elapsedTime) );  \n",
    "    return trainedModelGPU, elapsedTime\n",
    "\n",
    "def test_model_hpo ( trainedModelGPU, trainingTime, testData_cDF, testLabels_cDF ):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    testDMatrix = xgboost.DMatrix( data = testData_cDF, label = testLabels_cDF )    \n",
    "    predictionsGPU = trainedModelGPU.predict( testDMatrix ).astype(int)\n",
    "    \n",
    "    return predictionsGPU, trainedModelGPU.best_iteration, trainingTime, time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpo ( daskClient, nTimesteps, nParticles, nWorkers, paramRanges, trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF, randomSeed = 0):\n",
    "    \n",
    "    pandasTestLabels = testLabels_cDF.to_pandas()\n",
    "\n",
    "    if daskClient is not None:        \n",
    "        scatteredData_future = daskClient.scatter( [ trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF ], broadcast = True )\n",
    "    \n",
    "    trainData_cDF_future = scatteredData_future[0]; trainLabels_cDF_future = scatteredData_future[1]\n",
    "    testData_cDF_future = scatteredData_future[2]; testLabels_cDF_future = scatteredData_future[3]\n",
    "    \n",
    "    particles, velocities, accuracies, bestParticleIndex, particleColors = rl.initalize_hpo ( nTimesteps = nTimesteps, \n",
    "                                                                                              nParticles = nParticles, \n",
    "                                                                                              nWorkers = nWorkers, \n",
    "                                                                                              paramRanges = paramRanges)\n",
    "    \n",
    "    particleBoostingRounds = np.zeros((nTimesteps, nParticles)) # todo fix\n",
    "    trainingTimes = np.zeros (( nTimesteps, nParticles ))\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    for iTimestep in range (0, nTimesteps ):    \n",
    "        \n",
    "        \n",
    "        if daskClient is not None:\n",
    "            # [ delayed ] train xgboost models on train data\n",
    "            delayedParticleTrain = [ delayed( train_model_hpo )( trainData_cDF_future, trainLabels_cDF_future, \n",
    "                                                                     particles[iTimestep, iParticle, : ], \n",
    "                                                                         iParticle, iTimestep) for iParticle in range(nParticles) ]\n",
    "\n",
    "            # [ delayed ] determine number of trees/training-rounds returned early stopping -- used to set particle sizes\n",
    "            delayedParticleRounds = [ iParticle[0].best_iteration for iParticle in delayedParticleTrain ]\n",
    "            \n",
    "            # [delayed ] eval trained models on test/validation data\n",
    "            delayedParticlePredictions = [ delayed( test_model_hpo )(iParticle[0], iParticle[1], \n",
    "                                                                         testData_cDF_future, testLabels_cDF_future) for iParticle in delayedParticleTrain ]\n",
    "                        \n",
    "            # execute delayed             \n",
    "            particlePredictions = dask.compute( delayedParticlePredictions )[0]            \n",
    "            \n",
    "            # compute accuracies of predictions\n",
    "            accuracies[iTimestep, :] = [ accuracy_score ( pandasTestLabels, iParticle[0]) for iParticle in particlePredictions ]\n",
    "            particleBoostingRounds[iTimestep, : ] = [ iParticle[1] for iParticle in particlePredictions ]\n",
    "            trainingTimes[iTimestep, :] = [ iParticle[2] for iParticle in particlePredictions ]\n",
    "            \n",
    "        else:\n",
    "            for iParticle in range(nParticles):\n",
    "                trainedModels, _ = train_model_hpo ( pandasTrainData, pandasTrainLabels, particles[iTimestep, iParticle, : ], iParticle, iTimestep)\n",
    "                predictions, _ = test_model_hpo( trainedModels, pandasTestData, pandasTestLabels)            \n",
    "                accuracies[iTimestep, iParticle] = accuracy_score (pandasTestLabels, predictions)\n",
    "        \n",
    "        bestParticleIndex[iTimestep+1] = np.argmax( accuracies[iTimestep, :] )\n",
    "        print('@ hpo timestep : {}, best accuracy is {}'.format(iTimestep, np.max(accuracies[iTimestep, :])) )\n",
    "        if iTimestep +1 < nTimesteps:\n",
    "            particles[iTimestep+1, :, :], velocities[iTimestep+1, :, : ] = update_particles ( paramRanges, \n",
    "                                                                                              particles[iTimestep, :, :].copy(),\n",
    "                                                                                              velocities[iTimestep, :, :].copy(), \n",
    "                                                                                              bestParticleIndex[iTimestep+1])\n",
    "    \n",
    "    particleSizes = particleBoostingRounds/np.max(particleBoostingRounds)*10 + 2 # todo fix\n",
    "    elapsedTime = time.time() - startTime\n",
    "    print( 'elapsed time : {}'.format(elapsedTime) )\n",
    "    \n",
    "    return accuracies, particles, velocities, particleSizes, particleColors, bestParticleIndex, particleBoostingRounds, trainingTimes, elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTimesteps = 10\n",
    "nWorkers = 4\n",
    "nParticles = 2*nWorkers**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 3, 15, 'int'],\n",
    "                1: ['learning_rate', .01, 5, 'float'],\n",
    "                2: ['lambda', .01, 10, 'float'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, particles, velocities, particleSizes, particleColors, \\\n",
    "bestParticleIndex, particleBoostingRounds, trainingTimes, elapsedTime = run_hpo ( client, nTimesteps, \n",
    "                                                                                  nParticles,\n",
    "                                                                                  nWorkers,\n",
    "                                                                                  paramRanges, \n",
    "                                                                                  trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find top performing parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParamIndex = np.unravel_index(np.argmax(accuracies, axis=None), accuracies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuracy\n",
    "print('highest accuracy               :  {} '.format(accuracies[bestParamIndex[0], bestParamIndex[1]], bestParamIndex[0]+1))\n",
    "print('   @ timestep {}, particle {} '.format( bestParamIndex[0], bestParamIndex[1]))\n",
    "\n",
    "print('\\nbest model tree depth          :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 0]))\n",
    "print('best model learning rate       :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 1]))\n",
    "print('best model regularization      :  {} '.format(particles[bestParamIndex[0], bestParamIndex[1], 2]))\n",
    "print('best model num boosting rounds :  {} '.format(int(particleBoostingRounds[bestParamIndex[0], bestParamIndex[1]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.viz_search( accuracies, particleBoostingRounds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.hpo_animate (particles, particleSizes, particleColors, paramRanges, nTimesteps = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use best model params to train a model [ & visualize its mistakes ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forces workers to restart. useful to ensure GPU memory is clear\n",
    "client.restart()\n",
    "\n",
    "# limit work-stealing as much as possible\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.get('distributed.scheduler.work-stealing')\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 1})\n",
    "dask.config.get('distributed.scheduler.bandwidth')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/e/ec/ParticleSwarmArrowsAnimation.gif'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
