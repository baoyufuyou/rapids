{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Scaling XGBoost Hyper-Parameter Optimization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/swarm.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach highest performance in classification tasks (i.e., supervised learning ), it is best practice to build an ensemble of champion models. \n",
    "\n",
    "Each member of the ensemble is a winner of a search over many models of its kind with altered hyper-parameters.\n",
    "\n",
    "In this notebook, we build a harness for running such a [hyper-parameter] search to demonstrate the accuracy benefits while exploring performance as we scale within and accross GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np; import pandas as pd; import cudf\n",
    "import cuml; import xgboost; from xgboost import plot_tree\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import time; import copy \n",
    "\n",
    "import data_utils\n",
    "import swarm\n",
    "import visualization as viz\n",
    "\n",
    "# reload library modules/code without a kernel restart\n",
    "import importlib; importlib.reload( swarm ); importlib.reload( data_utils ); importlib.reload( viz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> In this notebook you can try different hyper-parameter search methods using synthetic or real data. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datasets.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion / Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 914 ms, total: 3.02 s\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dataset = data_utils.Dataset( 'fashion-mnist')\n",
    "# dataset = data_utils.Dataset( 'airline', nSamples = 1000000)\n",
    "# dataset = data_utils.Dataset( 'synthetic', coilType = 'helix', coilDensity = 9, nSamples = 5000000)\n",
    "dataset = data_utils.Dataset( 'synthetic', coilType = 'whirl', coilDensity = 18, nSamples = 5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "**Objective**: As is typical in machine learning workflows we need to split the dataset into a **train-set** and **test-set**. The test-set is unseen during training and model performance on the test-set is an indicator of how well our model(s) can generalize to future data [ also unseen during training ].\n",
    "\n",
    "\n",
    "> **Note**: In the case of synthetic data, we use a non-standard way of splitting the dataset in order to increase the difficulty of the problem and make it a challenge worthy of HPO. Specifically, we carve out the middle of the dataset along one of the synthetic dimensions [ x ], and assign these points to the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting synthetic dataset into train-set 75.0% and test-set 25.0%\n",
      "> assigning middle span of data to test-set\n",
      "> swapping 10000 samples between train-set and test-set\n"
     ]
    }
   ],
   "source": [
    "percentTrain = .75\n",
    "if dataset.datasetName != 'synthetic':    \n",
    "    trainData, testData, trainLabels, testLabels = cuml.train_test_split( dataset.data, dataset.labels, train_size = .75 )\n",
    "    \n",
    "else:    \n",
    "    # samples to exchange between train and test set [ enables generalization in the synthetic data case ]\n",
    "    samplesToSwap = int( dataset.data.shape[0] * .002 ) \n",
    "    # in the case of the helix increase the amount of training data since its a \n",
    "    if dataset.coilType == 'helix': percentTrain = .885 \n",
    "    trainData, testData, trainLabels, testLabels = data_utils.split_synthetic ( dataset.data, dataset.labels, \n",
    "                                                                                percentTrain = percentTrain, \n",
    "                                                                                samplesToSwap = samplesToSwap )\n",
    "\n",
    "dataset.assign_dataset_splits ( trainData, testData, trainLabels, testLabels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Apply standard scaling to each feature column of the dataset to ensure the data is numerically centered and ready for ingestion by an upstream model.\n",
    "**Note**: This is an inplace operation which changes the value of the argument dataframe passed in without returning a value.\n",
    "\n",
    ">More formally this transformation attempts to create normally distributed data features with 0 mean and unit variance.\n",
    "This is accomplished by computing the means and standard deviations of each column in the train-set, and applying these statistics to normalize both the train and test sets.\n",
    "\n",
    "> <font size=\"5\">$x_{rescaled} = \\frac{ x - \\mu_{train} }{\\sigma_{train}}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   x        y        z\n",
      "  train means  :  27.916    0.000    0.000\n",
      "  train stDevs :  18.782    7.764    7.679\n",
      "\n",
      "  test  means  :  29.302   -0.001    0.001\n",
      "  test  stDevs :   4.526    4.398    4.643\n"
     ]
    }
   ],
   "source": [
    "# before scaling\n",
    "data_utils.print_stats( dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying [inplace] standard scaling to train-set data\n",
      "applying [inplace] standard scaling to test-set data\n"
     ]
    }
   ],
   "source": [
    "# apply standard scaling inplace to train-set and test-set\n",
    "trainMeans, trainSTDevs, _ = data_utils.scale_dataframe_inplace ( dataset.trainData, label='train-set', datasetObject = dataset )\n",
    "_,_,_ = data_utils.scale_dataframe_inplace ( dataset.testData, trainMeans, trainSTDevs, label='test-set', datasetObject = dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   x        y        z\n",
      "  train means  :   0.000    0.000    0.000\n",
      "  train stDevs :   1.000    1.000    1.000\n",
      "\n",
      "  test  means  :   0.074   -0.000    0.000\n",
      "  test  stDevs :   0.241    0.566    0.605\n"
     ]
    }
   ],
   "source": [
    "# after scaling\n",
    "data_utils.print_stats( dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization \n",
    "\n",
    "**Objective**: Build intuition and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the dataset in its raw form [ no re-scaling ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting SYNTHETIC dataset, original shape: (5000000, 3)\n",
      " > plotting subset of 100000 samples -- 2.00% of total, adjust via maxSamplesToPlot \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c429f20db9a42a9b40e2177956e9613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.plot_data( dataset.data, dataset.labels, dataset.datasetName )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Train vs Test [ after split & rescaling ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualizing using train decimation factor: 148, test decimation factor: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f953c7afe1f41d3adec0ef9ceb64f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.plot_train_vs_test(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Hyper-Parameter Choice [ a.k.a., Can you beat HPO? ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexible parameters -- read more @ https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "manualXGBoostParams = { \n",
    "    'max_depth': 6,                     # default = 6             :: maximum depth of a tree\n",
    "    'num_boost_round': 50,              # default = XXX           :: number of trees        \n",
    "    'learning_rate': 0.3,               # default = 0.3           :: step size shrinkage between rounds, prevents overfitting\n",
    "    'gamma': 0.,                        # default = 0             :: minimum loss reduction required to make a leaf node split, prevents overfitting\n",
    "    'lambda': 1.,                       # default = 1             :: L2 regularizaiton term on weights, prevents overfitting\n",
    "    'alpha': 0.,                        # default = 0             :: L1 regularization term on weights, prevents overfitting\n",
    "    'tree_method': 'gpu_hist',          # default = 'gpu_hist'    :: tree construction algorithm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note that we'll inherit objective function from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy : 0.930 \n",
      "test accuracy  : 0.564 \n",
      "trained in 0.68 seconds\n",
      "\n",
      "parameter settings:\n",
      "\n",
      "{    'alpha': 0.0,\n",
      "     'gamma': 0.0,\n",
      "     'lambda': 1.0,\n",
      "     'learning_rate': 0.3,\n",
      "     'max_depth': 6,\n",
      "     'num_boost_round': 50,\n",
      "     'objective': 'binary:hinge',\n",
      "     'tree_method': 'gpu_hist'}\n"
     ]
    }
   ],
   "source": [
    "swarm.evaluate_manual_params( dataset, manualXGBoostParams )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dask.distributed import worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster( ip = '', n_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client( cluster, asynchronous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.3:33279</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.3:8787/status' target='_blank'>http://172.17.0.3:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>270.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.3:33279' processes=4 threads=4, memory=270.39 GB>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define HPO XGBoost Search Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: number of trees will be allowed to freely range between 0-2000 with an early stopping settting of 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 2, 15, 'int'],\n",
    "                1: ['learning_rate', .001, 2, 'float'],\n",
    "                2: ['gamma', 0., 3., 'float'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "| method name | &nbsp;&nbsp;&nbsp; performance | &nbsp;&nbsp;&nbsp; search duration  |\n",
    "|-----------------------|-----------------|------------------|\n",
    "| random-search         | &nbsp;&nbsp;&nbsp; worst | &nbsp;&nbsp;&nbsp; slow    |\n",
    "| particle-search [1]      | &nbsp;&nbsp;&nbsp; good  | &nbsp;&nbsp;&nbsp; fast    |\n",
    "| async-particle-search | &nbsp;&nbsp;&nbsp; best  | &nbsp;&nbsp;&nbsp; fastest |\n",
    "\n",
    "<center>[1] https://en.wikipedia.org/wiki/Particle_swarm_optimization#Algorithm</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Search Strategy : Particle Swarm, Random Search, Grid Search\n",
    "# Sync vs Async [ Dask Task Stream ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/sync_vs_async.png' width='1000px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Params [ nParticles & nEpochs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Perf [ <- ]\n",
    "You'll have need to have launched the container w Port Open\n",
    "Connect via [ the http:// is important]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color = '#ffb500'> Synchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! initializing SyncSwarm, with 10 particles, and 8 epochs\n"
     ]
    }
   ],
   "source": [
    "syncSwarm = swarm.SyncSwarm( client, dataset, paramRanges, nParticles = 10, nEpochs = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,[2.e+00 1.e-03 0.e+00],[2.41396007 1.43106658 0.74138218]\n",
      "1,[7.         1.86518216 2.99712155],[12.92680508 -1.48675846 -1.58346614]\n",
      "2,[10.          1.86315019  1.45424729],[-8.18786597  1.79002698 -1.07678138]\n",
      "3,[12.          1.68005814  1.70793398],[-11.16115312  -1.51392833  -0.37762835]\n",
      "4,[12.          0.34621797  1.8271068 ],[10.41615784  1.42177255  0.58533723]\n",
      "5,[5.         1.66282434 2.938335  ],[-11.56531678  -0.54477989  -2.46107377]\n",
      "6,[12.          0.41960333  1.11032821],[11.63437799 -1.74211009  2.91017289]\n",
      "7,[6.         0.87738005 2.93396854],[7.2778886  0.89341378 0.23097522]\n",
      "8,[5.         0.47963971 2.44938385],[-12.71102454  -0.48968714  -0.46589519]\n",
      "9,[15.  2.  3.],[ 0.0487394  -1.46394954 -1.68864795]\n",
      "new best 0.50000 found by particle 0 on eval 0\n",
      "new best 0.94346 found by particle 1 on eval 1\n",
      "new best 0.94379 found by particle 2 on eval 2\n",
      "new best 0.94419 found by particle 3 on eval 3\n",
      "new best 0.95441 found by particle 4 on eval 4\n",
      "> sync epoch 0 of 8\n",
      "> sync epoch 1 of 8\n",
      "new best 0.95563 found by particle 6 on eval 26\n",
      "> sync epoch 2 of 8\n",
      "> sync epoch 3 of 8\n",
      "> sync epoch 4 of 8\n",
      "new best 0.95570 found by particle 0 on eval 50\n",
      "> sync epoch 5 of 8\n",
      "new best 0.95619 found by particle 4 on eval 64\n",
      "> sync epoch 6 of 8\n",
      "> sync epoch 7 of 8\n",
      "search completed...\n",
      "\n",
      "\n",
      "       accuracy : 0.95619\n",
      "   elapsed time : 102.636 seconds\n",
      "\n",
      "      parameter | opt. value\n",
      "----------------------------------\n",
      "      max_depth : 11\n",
      "  learning_rate : 0.259\n",
      "          gamma : 1.621\n",
      "         nTrees : 244\n"
     ]
    }
   ],
   "source": [
    "syncSwarm.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#ffb500'> Synchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( syncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( syncSwarm, topN = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( syncSwarm, syncSwarm.paramRanges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color='#7400ff'> Asynchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncSwarm = swarm.AsyncSwarm( client, dataset, paramRanges, nParticles = 10, nEpochs = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncSwarm.run_search( syncWarmupFlag = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#7400ff'> Asynchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( asyncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( asyncSwarm, topN = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( asyncSwarm, asyncSwarm.paramRanges )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color='#666666'> Random Search </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAsyncSwarm = swarm.RandomSearchAsync ( client, dataset, paramRanges, nParticles = 8, nEpochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAsyncSwarm.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#666666'> Random Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( randomAsyncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( randomAsyncSwarm, topN = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( randomAsyncSwarm, randomAsyncSwarm.paramRanges )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine best swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if syncSwarm.globalBest['accuracy'] > asyncSwarm.globalBest['accuracy']:\n",
    "    swarm = syncSwarm\n",
    "else:\n",
    "    swarm = asyncSwarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost model with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParams = {\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'random_state': 0, \n",
    "    'max_depth': int(swarm.globalBest['params'][0]),\n",
    "    'learning_rate': swarm.globalBest['params'][1],\n",
    "    'gamma': swarm.globalBest['params'][2]\n",
    "}\n",
    "    \n",
    "bestParams['objective'] = dataset.trainObjective[0]\n",
    "if dataset.trainObjective[1] is not None: \n",
    "    bestParams['num_class'] = dataset.trainObjective[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainDMatrix = xgboost.DMatrix( data = dataset.trainData, label = dataset.trainLabels )\n",
    "testDMatrix = xgboost.DMatrix( data = dataset.testData, label = dataset.testLabels )\n",
    "trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], params = bestParams,\n",
    "                                 num_boost_round = swarm.globalBest['nTrees'], verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "trainedModelGPU.save_model('xgb.model.hpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import ForestInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = ForestInference.load( filename='xgb.model.hpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = ForestInference.load( filename='xgb.model.hpo',\n",
    "                           algo='BATCH_TREE_REORG',\n",
    "                           output_class=True,\n",
    "                           threshold=0,\n",
    "                           model_type='xgboost' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( dataset.testData )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.testData.as_gpu_matrix(order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iColumn in dataset.testData.columns:    \n",
    "    dataset.testData[iColumn] = dataset.testData[iColumn].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.testData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainedModelGPU.predict( testDMatrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filPredictions = fm.predict ( dataset.testData, \n",
    "                              algo='BATCH_TREE_REORG', \n",
    "                              output_class=True, \n",
    "                              threshold=0, \n",
    "                              model_type='xgboost' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudf.Series(filPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform prediction on the model loaded from path\n",
    "fil_preds = fm.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = trainedModelGPU.eval(testDMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPerf = 1 - float( predictions.split(':')[1] )\n",
    "print(f'accuracy: {testDataPerf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Up Results [ DGX-2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/synthetic_async.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async Scaling > Sync Scaling > Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.osgeo.cn/matplotlib/gallery/lines_bars_and_markers/timeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work / Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "larger than single GPU memory datasets - dask_cudf + [ dask_xgboost or xgboost.dask ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ Generate a classification dataset on GPU ](#data-load) (e.g., double helix, unwinding helix/whirl )\n",
    "\n",
    "2. [ ETL - process/prepare data for model training ](#ETL) (e.g., scale, split, augment )   \n",
    "    \n",
    "3. [ Define HPO Strategy ](#define-hpo)\n",
    "\n",
    "4. [ Create Compute Cluster ](#compute-cluster)\n",
    "   > LocalCUDACluster or KubeCluster\n",
    "      \n",
    "5. [ Define Seach ](#define-search)\n",
    "\n",
    "6. [ Run ASYNC Particle Swarm ](#run-async-PSO)\n",
    "\n",
    "7. [ Run Classic Particle Swarm ](#run-classic-PSO)\n",
    "\n",
    "8. [ Run Random Search Baseline ](#run-random-search)\n",
    "\n",
    "9. [ Summary ](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Choices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is able to make several key choices in running this notebook. They are as follows:\n",
    "\n",
    "1. [ Dataset ]()\n",
    "2. [ Compute Scaling Strategy - Scale-Up, Scale-Out ]()\n",
    "3. [ XGBoost Parameter Search Range ]()\n",
    "\n",
    "4. [ Particle Swarm Type ]()\n",
    "   * Synchronous\n",
    "   * Asynchronous\n",
    "   * Random Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
