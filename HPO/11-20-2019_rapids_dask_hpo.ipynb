{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rapids motivation](images/rapids_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <center>Scaling Hyper-Parameter Optimization with RAPIDS + Dask + [ Kubernetes ]</center>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:**\n",
    "\n",
    "To reach highest performance in classification tasks (i.e., supervised learning ), it is best practice to build an ensemble of champion models. \n",
    "\n",
    "Each member of the ensemble is a winner of a search over many models of its kind with altered hyper-parameters.\n",
    "\n",
    "In this notebook, we build a harness for running such a [hyper-parameter] search to demonstrate the accuracy benefits while exploring performance as we scale within and accross GPU nodes.\n",
    "\n",
    "**Choices:**\n",
    "The user is able to make several key choices in running this notebook. They are as follows:\n",
    "\n",
    "1. Dataset\n",
    "2. Model Type & Search Bounds\n",
    "3. Search Strategy\n",
    "   * Particle Swarm - Fully Asynchronous or Sync at Epochs\n",
    "   * Random Search\n",
    "   * Grid Search\n",
    "4. Compute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> In this notebook you can try different hyper-parameter search methods using synthetic or real data. </center>\n",
    " \n",
    "&nbsp;\n",
    "\n",
    "| method name | &nbsp;&nbsp;&nbsp; performance | &nbsp;&nbsp;&nbsp; search duration  |\n",
    "|-----------------------|-----------------|------------------|\n",
    "| random-search         | &nbsp;&nbsp;&nbsp; worst | &nbsp;&nbsp;&nbsp; slow    |\n",
    "| particle-search [1]      | &nbsp;&nbsp;&nbsp; good  | &nbsp;&nbsp;&nbsp; fast    |\n",
    "| async-particle-search | &nbsp;&nbsp;&nbsp; best  | &nbsp;&nbsp;&nbsp; fastest |\n",
    "\n",
    "<center>[1] https://en.wikipedia.org/wiki/Particle_swarm_optimization#Algorithm</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ Generate a classification dataset on GPU ](#data-load) (e.g., double helix, unwinding helix/whirl )\n",
    "\n",
    "2. [ ETL - process/prepare data for model training ](#ETL) (e.g., scale, split, augment )   \n",
    "    \n",
    "3. [ Define HPO Strategy ](#define-hpo)\n",
    "\n",
    "4. [ Create Compute Cluster ](#compute-cluster)\n",
    "   > LocalCUDACluster or KubeCluster\n",
    "      \n",
    "5. [ Define Seach ](#define-search)\n",
    "\n",
    "6. [ Run ASYNC Particle Swarm ](#run-async-PSO)\n",
    "\n",
    "7. [ Run Classic Particle Swarm ](#run-classic-PSO)\n",
    "\n",
    "8. [ Run Random Search Baseline ](#run-random-search)\n",
    "\n",
    "9. [ Summary ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np; import pandas as pd; import cudf\n",
    "import cuml; import xgboost; from xgboost import plot_tree\n",
    "\n",
    "import time; import copy \n",
    "\n",
    "import data_utils               # load datasets (or generate data) on the gpu\n",
    "import swarm                    # particle swarm implementation\n",
    "import visualization as viz     # visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload library modules/code without a kernel restart\n",
    "import importlib; importlib.reload( swarm ); importlib.reload( data_utils ); importlib.reload( viz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: update viz\n",
    "\n",
    "> TODO: prune swarm.py, data_utils.py, visualization.py\n",
    "\n",
    "> TODO: main.py / command-line\n",
    "\n",
    "> TODO: run_local_experiment.py -> scaling_experiments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dask.distributed import worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 3, 20, 'int'],\n",
    "                1: ['learning_rate', .001, 1, 'float'],\n",
    "                2: ['gamma', 0, 2, 'float'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataset():    \n",
    "    def __init__(self, datasetName = 'synthetic', nSamples = None):\n",
    "        self.datasetName = datasetName\n",
    "        \n",
    "        if self.datasetName == 'synthetic':\n",
    "            \n",
    "            if nSamples == None: nSamples = 1000000\n",
    "            data, labels, elapsedTime  = data_utils.generate_dataset( coilType = 'helix', nSamples = nSamples)\n",
    "            self.trainObjective = ['binary:hinge', None]\n",
    "\n",
    "        elif self.datasetName == 'fashion-mnist':\n",
    "\n",
    "            data, labels, elapsedTime = data_utils.load_fashion_mnist () \n",
    "            self.trainObjective = ['multi:softmax', 10]\n",
    "\n",
    "        elif self.datasetName == 'airline':\n",
    "            \n",
    "            if nSamples == None: nSamples = 5000000\n",
    "            data, labels, elapsedTime = data_utils.load_airline_dataset ( 'data/', np.min( ( nSamples, 115000000 )))\n",
    "            self.trainObjective = ['binary:hinge', None]\n",
    "        \n",
    "        # split train and test data\n",
    "        self.trainData, self.trainLabels, self.testData, self.testLabels = self.split_train_test ( data, labels )\n",
    "        \n",
    "        # apply standard scaling\n",
    "        trainMeans, trainSTDevs, _ = data_utils.scale_dataframe_inplace ( self.trainData )\n",
    "        _,_,_ = data_utils.scale_dataframe_inplace ( self.testData, trainMeans, trainSTDevs )\n",
    "        \n",
    "    def split_train_test(self, data, labels, trainSize = .75, trainTestOverlap = .025 ):\n",
    "        if self.datasetName == 'synthetic':            \n",
    "            trainData, trainLabels, testData, testLabels, _ = data_utils.split_train_test_nfolds ( data, labels, trainTestOverlap = trainTestOverlap )\n",
    "        else:\n",
    "            trainData, testData, trainLabels, testLabels = cuml.train_test_split( data, labels, shuffle = False, train_size= trainSize )        \n",
    "        return trainData, trainLabels, testData, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Particle():\n",
    "    def __init__ (self, pos, velo, particleID = -1 ):\n",
    "        self.pos = pos\n",
    "        self.velo = velo\n",
    "        self.pID = particleID\n",
    "        self.personalBestParams = pos\n",
    "        self.personalBestPerf = 0\n",
    "        self.posHistory = []\n",
    "        self.evalTimeHistory = []\n",
    "        self.nEvals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Swarm():\n",
    "    def __init__ ( self, client, dataset, paramRanges, nParticles = 10, nEpochs = 10 ):\n",
    "        \n",
    "        self.client = client\n",
    "        self.dataset = dataset\n",
    "        self.paramRanges = paramRanges\n",
    "        \n",
    "        self.nParticles = nParticles\n",
    "        self.nEpochs = nEpochs\n",
    "        self.reset_swarm()\n",
    "        \n",
    "    def reset_swarm( self ):\n",
    "        self.nEvals = 0\n",
    "        \n",
    "        self.particles = {}\n",
    "        self.delayedEvalParticles = []\n",
    "\n",
    "        self.globalBest = {'accuracy': 0, 'particleID': -1, 'params': [], 'iEvaluation': - 1}\n",
    "    \n",
    "    def scatter_data_to_workers( self ):\n",
    "        self.scatteredDataFutures = None\n",
    "        if self.client is not None:\n",
    "            self.scatteredDataFutures = self.client.scatter( [ self.dataset.trainData, self.dataset.trainLabels,\n",
    "                                                               self.dataset.testData,  self.dataset.testLabels ], broadcast = True )\n",
    "    \n",
    "    def build_initial_particles( self ):\n",
    "        self.delayedEvalParticles = []\n",
    "        for iParticle in range( self.nParticles ):\n",
    "            pos, velo = swarm.sample_params ( self.paramRanges )\n",
    "            self.particles[iParticle] = Particle( pos, velo, particleID = iParticle )\n",
    "            self.delayedEvalParticles.append ( delayed ( evaluate_particle ) ( self.scatteredDataFutures,\n",
    "                                                                               self.particles[iParticle].pos,\n",
    "                                                                               self.paramRanges,\n",
    "                                                                               self.particles[iParticle].pID,\n",
    "                                                                               self.dataset.trainObjective ) )\n",
    "\n",
    "    def enforce_bounds( self, newParameters ):\n",
    "        for iParameter in range( len ( newParameters )):\n",
    "            newParameters[iParameter] = np.clip ( newParameters[iParameter], self.paramRanges[iParameter][1], self.paramRanges[iParameter][2])\n",
    "        return newParameters\n",
    "    \n",
    "    def update_particle (self, pID, latestTestDataPerf, evalTime, \n",
    "                         wMomentum = .1, wGlobalBest = .55, wPersonalBest = .35, \n",
    "                         mode = 'classification'):\n",
    "        \n",
    "        if latestTestDataPerf > self.globalBest['accuracy']:\n",
    "            self.globalBest['accuracy'] = latestTestDataPerf\n",
    "            self.globalBest['params'] = self.particles[pID].pos.copy()\n",
    "            self.globalBest['particleID'] = pID\n",
    "            print(f'new global best {latestTestDataPerf:0.5f} found by particle {pID}, at eval {self.nEvals}')\n",
    "        \n",
    "        if latestTestDataPerf > self.particles[pID].personalBestPerf:\n",
    "            self.particles[pID].personalBestPerf = latestTestDataPerf\n",
    "            self.particles[pID].personalBestParams = self.particles[pID].pos.copy()\n",
    "            print(f'\\t\\t new personal best {latestTestDataPerf:0.5f} found by particle {pID}, at eval {self.nEvals}')\n",
    "        \n",
    "        # computing update terms for particle swarm\n",
    "        inertiaInfluence = self.particles[pID].velo.copy()\n",
    "        socialInfluence = ( self.globalBest['params'] - self.particles[pID].pos )\n",
    "        individualInfluence = ( self.particles[pID].personalBestParams - self.particles[pID].pos )\n",
    "\n",
    "        self.particles[pID].velo  =    wMomentum      *  inertiaInfluence     \\\n",
    "                                     + wPersonalBest  *  individualInfluence  * np.random.random()   \\\n",
    "                                     + wGlobalBest    *  socialInfluence      * np.random.random()\n",
    "\n",
    "        self.particles[pID].pos = self.particles[pID].pos.copy() + self.particles[pID].velo\n",
    "        self.particles[pID].pos = self.enforce_bounds( self.particles[pID].pos )\n",
    "        \n",
    "        self.particles[pID].posHistory.append( self.particles[pID].pos )\n",
    "        self.particles[pID].nEvals += 1\n",
    "        self.particles[pID].evalTimeHistory.append( evalTime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SyncSwarm ( Swarm ):\n",
    "    def run_search( self ):\n",
    "        self.reset_swarm ()\n",
    "        self.scatter_data_to_workers ()\n",
    "        self.build_initial_particles ()\n",
    "        \n",
    "        for iEpoch in range( self.nEpochs ):\n",
    "            futureEvalParticles = self.client.compute( self.delayedEvalParticles )\n",
    "            self.delayedEvalParticles = []\n",
    "            \n",
    "            for iParticleFuture in futureEvalParticles:\n",
    "                testDataPerf, trainDataPerf, pID, evalTime = iParticleFuture.result()\n",
    "                self.update_particle ( pID, testDataPerf, evalTime ) # inplace update to particle.pos, particle.velo\n",
    "                \n",
    "                self.nEvals += 1\n",
    "                \n",
    "                self.delayedEvalParticles.append ( delayed ( evaluate_particle ) ( self.scatteredDataFutures,\n",
    "                                                                                   self.particles[pID].pos,\n",
    "                                                                                   self.paramRanges,\n",
    "                                                                                   self.particles[pID].pID,\n",
    "                                                                                   self.dataset.trainObjective ) )\n",
    "class AsyncSwarm ( Swarm ):\n",
    "    def run_search( self ):\n",
    "        self.reset_swarm ()\n",
    "        self.scatter_data_to_workers ()\n",
    "        self.build_initial_particles ()\n",
    "\n",
    "        futureEvalParticles = self.client.compute( self.delayedEvalParticles )\n",
    "        particleFutureSeq = as_completed( futureEvalParticles )\n",
    "        \n",
    "        # note that the particleFutureSeq is an iterator of futures\n",
    "        # at the end of the loop we create new work and append it to the iterartor, this behavior resembles a while loop\n",
    "        # see dask documentation https://docs.dask.org/en/latest/futures.html#distributed.as_completed\n",
    "        for particleFuture in particleFutureSeq: \n",
    "            testDataPerf, trainDataPerf, pID, evalTime = particleFuture.result()\n",
    "            self.update_particle ( pID, testDataPerf, evalTime ) # inplace update to particle.pos, particle.velo\n",
    "\n",
    "            self.nEvals += 1\n",
    "            approximateEpoch = self.nEvals // self.nParticles\n",
    "            if approximateEpoch > self.nEpochs: break\n",
    "            \n",
    "            delayedParticle = delayed ( evaluate_particle ) ( self.scatteredDataFutures,\n",
    "                                                              self.particles[pID].pos,\n",
    "                                                              self.paramRanges,\n",
    "                                                              self.particles[pID].pID,\n",
    "                                                              self.dataset.trainObjective )\n",
    "            \n",
    "            futureParticle = self.client.compute( delayedParticle )\n",
    "            particleFutureSeq.add( futureParticle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# xgboost parameters -- https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "def evaluate_particle ( scatteredDataFutures, particleParams, paramRanges, particleID, trainObjective, earlyStoppingRounds = 25 ) :\n",
    "    trainDataFuture = scatteredDataFutures[0]\n",
    "    trainLabelsFuture = scatteredDataFutures[1]\n",
    "    testDataFuture = scatteredDataFutures[2]\n",
    "    testLabelsFuture = scatteredDataFutures[3]\n",
    "        \n",
    "    xgboostParams = {\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': 0, \n",
    "    }\n",
    "    \n",
    "    # objective [ binary or multi-class ]\n",
    "    xgboostParams['objective'] = trainObjective[0]\n",
    "    if trainObjective[1] is not None: xgboostParams['num_class'] = trainObjective[1]\n",
    "    \n",
    "    def enforce_type( parameterValue, paramRange ):        \n",
    "        if paramRange[3] == 'int': \n",
    "            return int( parameterValue )\n",
    "        elif paramRange[3] == 'float':\n",
    "            return float( parameterValue )\n",
    "        \n",
    "    # flexible parameters\n",
    "    xgboostParams['max_depth'] = enforce_type( particleParams[0], paramRanges[0] )\n",
    "    xgboostParams['learning_rate'] = enforce_type( particleParams[1], paramRanges[1] ) # shrinkage of feature weights after each boosting step\n",
    "    xgboostParams['gamma'] = enforce_type( particleParams[2], paramRanges[2] ) # complexity control, range [0, Inf ]\n",
    "    xgboostParams['num_boost_rounds'] = 2000\n",
    "    \n",
    "    startTime = time.time()\n",
    "\n",
    "    trainDMatrix = xgboost.DMatrix( data = trainDataFuture, label = trainLabelsFuture )\n",
    "    testDMatrix = xgboost.DMatrix( data = testDataFuture, label = testLabelsFuture )\n",
    "\n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], params = xgboostParams,\n",
    "                                     num_boost_round = xgboostParams['num_boost_rounds'], \n",
    "                                     early_stopping_rounds = earlyStoppingRounds,\n",
    "                                     verbose_eval = False )\n",
    "\n",
    "    trainDataPerf = 1 - float( trainedModelGPU.eval(trainDMatrix).split(':')[1] )\n",
    "    testDataPerf = 1 - float( trainedModelGPU.eval(testDMatrix).split(':')[1] )   \n",
    "\n",
    "    elapsedTime = time.time() - startTime\n",
    "\n",
    "    return testDataPerf, trainDataPerf, particleID, elapsedTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster( ip = '', n_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client( cluster, asynchronous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:45177</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8787/status' target='_blank'>http://172.17.0.2:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>270.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:45177' processes=4 threads=4, memory=270.39 GB>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating blobs; # points = 500000\n",
      "generating blobs; # points = 500000\n",
      "splitting data into training and test set\n",
      "rescaling data\n",
      "rescaling data\n"
     ]
    }
   ],
   "source": [
    "#s = SyncSwarm( client, Dataset('synthetic'), paramRanges, nEpochs = 3 )\n",
    "s = swarm.AsyncSwarm( client, data_utils.Dataset('synthetic'), paramRanges, nEpochs = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new global best 0.22356 found by particle 7, at eval 0\n",
      "\t\t new personal best 0.22356 found by particle 7, at eval 0\n",
      "new global best 0.89590 found by particle 6, at eval 1\n",
      "\t\t new personal best 0.89590 found by particle 6, at eval 1\n",
      "new global best 0.92141 found by particle 1, at eval 2\n",
      "\t\t new personal best 0.92141 found by particle 1, at eval 2\n",
      "new global best 0.92362 found by particle 4, at eval 3\n",
      "\t\t new personal best 0.92362 found by particle 4, at eval 3\n",
      "\t\t new personal best 0.92168 found by particle 2, at eval 5\n",
      "\t\t new personal best 0.28172 found by particle 5, at eval 6\n",
      "new global best 0.92551 found by particle 8, at eval 7\n",
      "\t\t new personal best 0.92551 found by particle 8, at eval 7\n",
      "\t\t new personal best 0.91869 found by particle 6, at eval 8\n",
      "\t\t new personal best 0.91586 found by particle 0, at eval 9\n",
      "\t\t new personal best 0.92333 found by particle 2, at eval 10\n",
      "new global best 0.92803 found by particle 7, at eval 11\n",
      "\t\t new personal best 0.92803 found by particle 7, at eval 11\n",
      "\t\t new personal best 0.92667 found by particle 4, at eval 12\n",
      "new global best 0.92867 found by particle 5, at eval 13\n",
      "\t\t new personal best 0.92867 found by particle 5, at eval 13\n",
      "\t\t new personal best 0.92759 found by particle 8, at eval 14\n",
      "\t\t new personal best 0.88706 found by particle 3, at eval 15\n",
      "\t\t new personal best 0.92662 found by particle 6, at eval 16\n",
      "\t\t new personal best 0.92787 found by particle 9, at eval 18\n",
      "\t\t new personal best 0.92685 found by particle 0, at eval 19\n",
      "\t\t new personal best 0.90792 found by particle 3, at eval 20\n",
      "\t\t new personal best 0.92835 found by particle 8, at eval 21\n",
      "\t\t new personal best 0.92514 found by particle 3, at eval 23\n",
      "\t\t new personal best 0.92767 found by particle 6, at eval 24\n",
      "new global best 0.92993 found by particle 9, at eval 26\n",
      "\t\t new personal best 0.92993 found by particle 9, at eval 26\n",
      "new global best 0.93052 found by particle 3, at eval 27\n",
      "\t\t new personal best 0.93052 found by particle 3, at eval 27\n",
      "\t\t new personal best 0.92380 found by particle 2, at eval 28\n",
      "new global best 0.93128 found by particle 7, at eval 29\n",
      "\t\t new personal best 0.93128 found by particle 7, at eval 29\n",
      "new global best 0.93142 found by particle 5, at eval 35\n",
      "\t\t new personal best 0.93142 found by particle 5, at eval 35\n",
      "\t\t new personal best 0.92453 found by particle 2, at eval 36\n",
      "\t\t new personal best 0.92911 found by particle 6, at eval 38\n",
      "\t\t new personal best 0.93074 found by particle 8, at eval 40\n",
      "new global best 0.93190 found by particle 5, at eval 41\n",
      "\t\t new personal best 0.93190 found by particle 5, at eval 41\n",
      "\t\t new personal best 0.92728 found by particle 2, at eval 44\n",
      "new global best 0.93566 found by particle 5, at eval 45\n",
      "\t\t new personal best 0.93566 found by particle 5, at eval 45\n",
      "\t\t new personal best 0.93237 found by particle 7, at eval 46\n",
      "\t\t new personal best 0.93541 found by particle 6, at eval 48\n",
      "\t\t new personal best 0.92598 found by particle 1, at eval 54\n",
      "\t\t new personal best 0.93352 found by particle 3, at eval 58\n",
      "\t\t new personal best 0.92610 found by particle 1, at eval 59\n",
      "\t\t new personal best 0.93477 found by particle 2, at eval 60\n",
      "\t\t new personal best 0.93186 found by particle 1, at eval 65\n",
      "\t\t new personal best 0.92811 found by particle 0, at eval 71\n",
      "new global best 0.93660 found by particle 5, at eval 76\n",
      "\t\t new personal best 0.93660 found by particle 5, at eval 76\n",
      "\t\t new personal best 0.93193 found by particle 8, at eval 80\n",
      "\t\t new personal best 0.93397 found by particle 9, at eval 85\n",
      "\t\t new personal best 0.92900 found by particle 0, at eval 96\n",
      "\t\t new personal best 0.93317 found by particle 7, at eval 102\n"
     ]
    }
   ],
   "source": [
    "s.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEvalList = []\n",
    "evalTimeList = []\n",
    "pIDList = []\n",
    "for pID, particle in s.particles.items():\n",
    "    nEvalList.append ( particle.nEvals )\n",
    "    evalTimeList.append ( np.mean( particle.evalTimeHistory ) )\n",
    "    pIDList.append( pID )\n",
    "df = pd.DataFrame ( data = { 'nEvals' : nEvalList}, index = pIDList )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('nEvals', ascending=False).plot.bar( figsize = (15, 7), color=[ tuple(viz.rapidsColors[1]), tuple(viz.rapidsColors[0]) ] );\n",
    "plt.xlabel('pID'); plt.ylabel('nEvals');\n",
    "plt.title('Async Swarm Evaluations Per Particle');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv.figure()\n",
    "for iParticle in range(len(s.particles)):    \n",
    "    xyz=np.matrix(s.particles[iParticle].posHistory)\n",
    "    ipv.scatter(xyz[:,0].squeeze(),xyz[:,1].squeeze(),xyz[:,2].squeeze(), size=2, marker='sphere', color=viz.rapidsColors[iParticle])\n",
    "    ipv.plot(xyz[:,0].squeeze(),xyz[:,1].squeeze(),xyz[:,2].squeeze(), size=2, color=viz.rapidsColors[iParticle])\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: update animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def viz_particle_movement( particleHistory ):\n",
    "    \n",
    "    sortedBarHeightsDF = sorted_eval_frequency_per_particle ( particleHistory )\n",
    "    \n",
    "    particleHistoryCopy = copy.deepcopy( particleHistory )\n",
    "    \n",
    "    paramRanges = particleHistoryCopy['paramRanges']\n",
    "    particleColors = particleHistoryCopy['particleColors']\n",
    "    nParticles = particleHistory['nParticles']\n",
    "    initialParticleParams = particleHistoryCopy['initialParams']\n",
    "    \n",
    "    nAnimationFrames = max( sortedBarHeightsDF['nEvals'] )\n",
    "    particleXYZ = np.zeros( ( nAnimationFrames, nParticles, 3 ) )\n",
    "    lastKnownLocation = {}\n",
    "    \n",
    "    \n",
    "    # TODO: bestIterationNTrees\n",
    "    # particleSizes[ iFrame, iParticle ] = particleHistoryCopy[iParticle]['bestIterationNTrees'].pop(0).copy()\n",
    "    \n",
    "    for iFrame in range( nAnimationFrames ):\n",
    "        for iParticle in range( nParticles ):\n",
    "            if iParticle in particleHistoryCopy.keys():\n",
    "                # particle exists in the particleHistory and it has parameters for the current frame\n",
    "                if len( particleHistoryCopy[iParticle]['particleParams'] ):\n",
    "                    particleXYZ[iFrame, iParticle, : ] = particleHistoryCopy[iParticle]['particleParams'].pop(0).copy()\n",
    "                    lastKnownLocation[iParticle] = particleXYZ[iFrame, iParticle, : ].copy()\n",
    "                else:\n",
    "                    # particle exists but it's params have all been popped off -- use its last known location\n",
    "                    particleXYZ[iFrame, iParticle, : ] = lastKnownLocation[iParticle].copy()\n",
    "                    \n",
    "            else:\n",
    "                # particle does not exist in the particleHistory\n",
    "                if iParticle in lastKnownLocation.keys():\n",
    "                    # particle has no params in current frame, attempting to use last known location\n",
    "                    particleXYZ[iFrame, iParticle, : ] = lastKnownLocation[iParticle].copy()                    \n",
    "                else:\n",
    "                    # using initial params\n",
    "                    particleXYZ[iFrame, iParticle, : ] = initialParticleParams[iParticle].copy()\n",
    "                    lastKnownLocation[iParticle] = particleXYZ[iFrame, iParticle, : ].copy()                    \n",
    "        \n",
    "    ipv.figure()\n",
    "    \n",
    "    colorStack = np.random.random( ( nParticles, 3) )\n",
    "    scatterPlots = ipv.scatter( particleXYZ[:, :,0], \n",
    "                                particleXYZ[:, :,1], \n",
    "                                particleXYZ[:, :,2], \n",
    "                                marker='sphere', \n",
    "                                size=5,\n",
    "                                color = particleColors )\n",
    "    \n",
    "    ipv.animation_control( [ scatterPlots ] , interval = 400 )\n",
    "    ipv.xlim( paramRanges[0][1]-.5, paramRanges[0][2]+.5 )\n",
    "    ipv.ylim( paramRanges[1][1]-.1, paramRanges[1][2]+.1 )\n",
    "    ipv.zlim( paramRanges[2][1]-.1, paramRanges[2][2]+.1 )\n",
    "    \n",
    "    ipv.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'data-load'></a>\n",
    "----\n",
    "# 1. Load/Generate Data \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with a dataset (data + binary labels) that can either be generated or loaded.\n",
    "\n",
    "Note that you are also welcome to bring your own dataset. The available datasets we provide include:\n",
    "\n",
    "| Name                                                      | Default Samples &nbsp;&nbsp;&nbsp;| Max Samples &nbsp;&nbsp;&nbsp;| Columns | Accuracy Before HPO  | Accuracy After HPO |\n",
    "|-----------------------------------------------------------|-----------------|-------------|---------|-------| -------|\n",
    "| synthetic helix/whirl &nbsp;&nbsp;&nbsp;                  | 1M              | Inf         | 3       | .25 | .95 |\n",
    "| [fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)    | 60K              | 60K         | 784      | .86 | .90 |\n",
    "| [airline](http://kt.ijs.si/elena_ikonomovska/data.html)   | 5M              | 115M        | 13      | .81 | .86 |\n",
    "\n",
    "<!--- | [higgs](https://archive.ics.uci.edu/ml/datasets/HIGGS)    | 1M              | 11M         | 28      | .69 | .75 | -->\n",
    "\n",
    "\n",
    "In addition to specifying a dataset, you can also choose the number of samples that will be loaded/generated -- this is helpful to make for a compelling demo in a short amount of time as well as to stay within memory limits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'airline'; nSamples = 5000000\n",
    "datasetName = 'synthetic'; nSamples = 1000000\n",
    "datasetName = 'fashion-mnist'; nSamples = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetName == 'synthetic':\n",
    "        \n",
    "    data, labels, elapsedTime  = data_utils.generate_dataset( coilType = 'helix', nSamples = nSamples)\n",
    "\n",
    "elif datasetName == 'fashion-mnist':\n",
    "    \n",
    "    data, labels, elapsedTime = data_utils.load_fashion_mnist () \n",
    "\n",
    "elif datasetName == 'airline':\n",
    "    \n",
    "    nSamplesToLoad = np.min( ( nSamples, 115000000 ))    \n",
    "    data, labels, elapsedTime = data_utils.load_airline_dataset ( 'data/', nSamplesToLoad)\n",
    "\n",
    "'''\n",
    "elif datasetName == 'higgs-kaggle':\n",
    "    nSamplesToLoad = np.min( ( nSamples, 250000 ))\n",
    "    data, labels, elapsedTime = data_utils.load_higgs_kaggle ()\n",
    "    \n",
    "elif datasetName == 'higgs':\n",
    "    nSamplesToLoad = np.min( ( nSamples, 11000000 ))\n",
    "    data, labels, elapsedTime = data_utils.load_higgs_dataset ( 'data/', nSamplesToLoad)\n",
    "'''        \n",
    "\n",
    "print(f'dataset shape: {data.shape}\\n > loaded in {elapsedTime} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Plot Dataset\n",
    "\n",
    "In the case of the synthetic dataset, the feature dimensionality is 3, so we do no need to do any reduction prior to 3D plotting.\n",
    "\n",
    "For real datasets with many features, we first apply a dimensionality reduction method prior to plotting.\n",
    "\n",
    "In the cells below we demonstrate several approaches to dimensionality reduction available in RAPIDS [ cuml ] :\n",
    "\n",
    "* PCA (linear, unsupervised), \n",
    "* TSNE (non-linear, unsupervised), and \n",
    "* UMAP (non-linear, supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetName == 'synthetic':\n",
    "    viz.plot_data( data, labels, datasetName )\n",
    "else:\n",
    "    viz.plot_data( data, labels, datasetName, dimReductionMethod = 'PCA', maxSamplesForDimReduction = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetName != 'synthetic':\n",
    "    viz.plot_data( data, labels, datasetName, dimReductionMethod = 'UMAP', maxSamplesForDimReduction = 100000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetName != 'synthetic':\n",
    "    viz.plot_data( data, labels, datasetName, dimReductionMethod = 'TSNE', maxSamplesForDimReduction = 25000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ETL'></a>\n",
    "\n",
    "# 2. ETL\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Split Data into Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetName == 'synthetic':\n",
    "    trainTestOverlap = .025\n",
    "    trainData, trainLabels, testData, testLabels, _ = data_utils.split_train_test_nfolds ( data, labels, trainTestOverlap = trainTestOverlap )\n",
    "else:\n",
    "    trainData, testData, trainLabels, testLabels = cuml.train_test_split( data, labels, shuffle = False, train_size=.70 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Re-scale / Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataframe_inplace ( targetDF, trainMeans = {}, trainSTDevs = {} ):    \n",
    "    print('rescaling data')\n",
    "    sT = time.time()\n",
    "    for iCol in targetDF.columns:\n",
    "        \n",
    "        # omit scaling label column\n",
    "        if iCol == targetDF.columns[-1] == 'label': continue\n",
    "            \n",
    "        # compute means and standard deviations for each column [ should skip for test data ]\n",
    "        if iCol not in trainMeans.keys() and iCol not in trainSTDevs.keys():            \n",
    "            trainMeans[iCol] = targetDF[iCol].mean()\n",
    "            trainSTDevs[iCol] = targetDF[iCol].std()\n",
    "            \n",
    "        # apply scaling to each column\n",
    "        targetDF[iCol] = ( targetDF[iCol] - trainMeans[iCol] ) / ( trainSTDevs[iCol] + 1e-10 )\n",
    "        \n",
    "    return trainMeans, trainSTDevs, time.time() - sT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standard scaling\n",
    "trainMeans, trainSTDevs, t_scaleTrain = scale_dataframe_inplace ( trainData )\n",
    "_,_, t_scaleTest = scale_dataframe_inplace ( testData, trainMeans, trainSTDevs ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Train vs Test Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_data( trainData, trainLabels, datasetName, dimReductionMethod = 'UMAP', maxSamplesForDimReduction = 100000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_data( testData, testLabels, datasetName, dimReductionMethod = 'UMAP', maxSamplesForDimReduction = 100000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape, testData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape, testData.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost parameters -- https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "paramsGPU = { \n",
    "    'max_depth': 5,               \n",
    "    'tree_method': 'gpu_hist',\n",
    "    'random_state': 0, \n",
    "    'learning_rate': 0.3,         # shrinkage of feature weights after each boosting step\n",
    "    'gamma': 0,                   # complexity control, range [0, Inf ]\n",
    "    'subsample': 1,\n",
    "    'min_child_weight': 1\n",
    "}\n",
    "# fixed parameters\n",
    "nTrees = 10\n",
    "\n",
    "nClasses = labels[labels.columns[0]].nunique()\n",
    "if nClasses == 2:\n",
    "    paramsGPU['objective'] = 'binary:hinge'\n",
    "else:\n",
    "    paramsGPU['objective'] = 'multi:softmax'\n",
    "    paramsGPU['num_class'] = nClasses\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "trainDMatrix = xgboost.DMatrix( data = trainData, label = trainLabels )\n",
    "testDMatrix = xgboost.DMatrix( data = testData, label = testLabels )\n",
    "\n",
    "trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], params = paramsGPU,\n",
    "                                 num_boost_round = nTrees, verbose_eval = False )\n",
    "\n",
    "trainAccuracy = 1 - float( trainedModelGPU.eval(trainDMatrix).split(':')[1] )\n",
    "testAccuracy = 1 - float( trainedModelGPU.eval(testDMatrix).split(':')[1] )   \n",
    "\n",
    "predictionsGPU = trainedModelGPU.predict( testDMatrix ).astype(int)\n",
    "\n",
    "elapsedTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' test accuracy: {testAccuracy:.3f},\\n train accuracy: {trainAccuracy:.3f},\\n elapsed time: {elapsedTime:.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a sample tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "plot_tree(trainedModelGPU, num_trees=0, ax=plt.subplot(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_data( testData, testLabels, datasetName, predictionsGPU, dimReductionMethod = 'UMAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'define-hpo'></a>\n",
    "\n",
    "# 3. Define HPO Strategy\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Swarm : \n",
    "> Particles are randomly initialized, and change their position in hyper-parameter space using three terms \n",
    "1. momentum\n",
    "2. personal best \n",
    "3. global best\n",
    "<center>    \n",
    "$ velo_{t+1} = velo_t * w_{momentum} + (pos_{personal-best} - pos_{current}) * w_{personal} + (pos_{global-best} - pos_{current}) * w_{global} $\n",
    "   $ pos_{t+1} = pos_{t} + velo_{t+1} $\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Evaluation Logic \n",
    "> (i.e., Train & Eval using Changing Parameter Sets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_particle ( particle, dataFutures, earlyStoppingRounds, retainPredictionsFlag ):    \n",
    "\n",
    "    # fixed parameters\n",
    "    paramsGPU = { 'tree_method': 'gpu_hist',\n",
    "                  'n_gpus': 1,\n",
    "                  'random_state': 0 }\n",
    "\n",
    "    # TODO: loop over paramRanges instead of hard code\n",
    "    paramsGPU['max_depth'] = int( particle['params'][0] )\n",
    "    paramsGPU['learning_rate'] = particle['params'][1]\n",
    "    paramsGPU['gamma'] = particle['params'][2]\n",
    "    paramsGPU['num_boost_rounds'] = 2000\n",
    "\n",
    "    nClasses = dataFutures['trainLabels'][dataFutures['trainLabels'].columns[0]].nunique()\n",
    "    if nClasses == 2:\n",
    "        paramsGPU['objective'] = 'binary:hinge'\n",
    "    else:\n",
    "        paramsGPU['objective'] = 'multi:softmax'\n",
    "        paramsGPU['num_class'] = nClasses        \n",
    "    \n",
    "    startTime = time.time()\n",
    "\n",
    "    trainDMatrix = xgboost.DMatrix( data = dataFutures['trainData'], label = dataFutures['trainLabels'] )\n",
    "    testDMatrix = xgboost.DMatrix( data = dataFutures['testData'], label = dataFutures['testLabels'] )\n",
    "\n",
    "    trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], \n",
    "                                     params = paramsGPU,\n",
    "                                     num_boost_round = paramsGPU['num_boost_rounds'],\n",
    "                                     early_stopping_rounds = earlyStoppingRounds,\n",
    "                                     verbose_eval = False )\n",
    "        \n",
    "    predictionsGPU = trainedModelGPU.predict( testDMatrix ).astype(int)\n",
    "            \n",
    "    elapsedTime = time.time() - startTime\n",
    "\n",
    "    particle['nTrees'] = trainedModelGPU.best_iteration\n",
    "    particle['trainAccuracy'] = 1 - float( trainedModelGPU.eval(trainDMatrix).split(':')[1] )\n",
    "    particle['testAccuracy'] = 1 - float( trainedModelGPU.eval(testDMatrix).split(':')[1] )    \n",
    "    \n",
    "    particle['predictions'] = None\n",
    "    if retainPredictionsFlag: \n",
    "        particle['predictions'] = predictionsGPU\n",
    "    \n",
    "    return particle, elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Update Logic \n",
    "https://en.wikipedia.org/wiki/Particle_swarm_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_particle( particle, paramRanges, globalBestParams, personalBestParams, \n",
    "                     wMomentum, wIndividual, wSocial, wExplore, randomSearchMode = False, randomSeed = None ):\n",
    "    ''' \n",
    "    # TODO: debug dask caching [?] attempting to use a seed produces the same sequence of random samples\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)    \n",
    "    '''\n",
    "    \n",
    "    # baseline to compare swarm update versus random search\n",
    "    if randomSearchMode:        \n",
    "        sampledParams, sampledVelocities = swarm.sample_params( paramRanges )\n",
    "        return sampledParams, sampledVelocities\n",
    "        \n",
    "    # computing update terms for particle swarm\n",
    "    inertiaInfluence = particle['velocities'].copy()\n",
    "    socialInfluence = ( globalBestParams - particle['params'] )\n",
    "    individualInfluence = ( personalBestParams - particle['params'] )\n",
    "    \n",
    "    newParticleVelocities =    wMomentum    *  inertiaInfluence \\\n",
    "                             + wIndividual  *  individualInfluence  * np.random.random()   \\\n",
    "                             + wSocial      *  socialInfluence      * np.random.random()\n",
    "    \n",
    "    newParticleParams = particle['params'].copy() + newParticleVelocities\n",
    "    newParticleParams = swarm.enforce_param_bounds_inline ( newParticleParams, paramRanges )\n",
    "            \n",
    "    return newParticleParams, newParticleVelocities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO Run Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpo ( client, mode, paramRanges, trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF,\n",
    "              nParticles, nEpochs,             \n",
    "              wMomentum = .05, wIndividual = .35, wBest = .25, wExplore = .15, earlyStoppingRounds = 50,\n",
    "              terminationAccuracy = np.Inf, \n",
    "              randomSeed = 0, \n",
    "              plotFlag = False,\n",
    "              retainPredictionsFlag = False ):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # scatter data to all workers\n",
    "    # ----------------------------\n",
    "    if client is not None:\n",
    "        scatteredData_future = client.scatter( [ trainData_cDF, trainLabels_cDF, testData_cDF, testLabels_cDF], broadcast = True )\n",
    "        \n",
    "    dataFutures = { 'trainData'   : scatteredData_future[0], 'trainLabels' : scatteredData_future[1], \n",
    "                    'testData'    : scatteredData_future[2], 'testLabels'  : scatteredData_future[3] }\n",
    "    \n",
    "    # ----------------------------\n",
    "    # initialize HPO strategy \n",
    "    # ----------------------------        \n",
    "    def initialize_particle_futures ( nParticles, paramRanges, randomSeed, plotFlag ) :\n",
    "        initialParticleParams, initialParticleVelocities, globalBest, particleColors = swarm.initialize_particle_swarm ( nParticles, paramRanges, randomSeed, plotFlag )\n",
    "        # create particle futures using the initialization positions and velocities    \n",
    "        delayedEvalParticles = []\n",
    "        for iParticle in range(nParticles):\n",
    "            particle = { 'ID': iParticle, 'params': initialParticleParams[iParticle], 'velocities': initialParticleVelocities[iParticle], 'predictions': None }        \n",
    "            delayedEvalParticles.append( delayed ( evaluate_particle )( particle.copy(), dataFutures, earlyStoppingRounds, retainPredictionsFlag ))\n",
    "        return delayedEvalParticles, initialParticleParams, globalBest, particleColors\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # shared logic for particle evaluation and updates\n",
    "    # ------------------------------------------------\n",
    "    def eval_and_update ( particleFuture, delayedEvalParticles, particleHistory, paramRanges, globalBest, randomSearchMode, nEvaluations ):\n",
    "        # convert particle future to concrete result and collect returned values\n",
    "        particle, elapsedTime = particleFuture.result()\n",
    "\n",
    "        # update hpo strategy meta-parameters -- i.e. swarm global best and particle personal best\n",
    "        particleHistory, globalBest = swarm.update_bests ( particleHistory, particle, globalBest, nEvaluations, mode['randomSearch'] )\n",
    "\n",
    "        # update history with this particle's latest contribution/eval\n",
    "        particleHistory = swarm.update_history_dictionary ( particleHistory, particle, nEvaluations )\n",
    "\n",
    "        # update particle\n",
    "        if randomSearchMode:\n",
    "            personalBestParams = None\n",
    "        else:\n",
    "            personalBestParams = particleHistory[particle['ID']]['personalBestParams']\n",
    "            \n",
    "        particle['params'], particle['velocities'] = update_particle ( particle, paramRanges,\n",
    "                                                                       globalBest['params'], personalBestParams,\n",
    "                                                                       wMomentum, wIndividual, wBest, wExplore,\n",
    "                                                                       randomSearchMode = randomSearchMode, \n",
    "                                                                       randomSeed = particle['ID'] ) # repeatability\n",
    "        return particle.copy(), particleHistory, globalBest\n",
    "    \n",
    "    nEvaluations = 0\n",
    "    particleHistory = {}\n",
    "    \n",
    "    if mode['allowAsyncUpdates'] != True:\n",
    "        # ----------------------------\n",
    "        # synchronous particle swarm\n",
    "        # ----------------------------\n",
    "        delayedEvalParticles, initialParticleParams, globalBest, particleColors = initialize_particle_futures ( nParticles, paramRanges, randomSeed, plotFlag )\n",
    "        futureEvalParticles = client.compute( delayedEvalParticles )\n",
    "        \n",
    "        for iEpoch in range (0, nEpochs ):    \n",
    "            futureEvalParticles = client.compute( delayedEvalParticles )\n",
    "            delayedEvalParticles = []\n",
    "            for particleFuture in futureEvalParticles:\n",
    "                newParticle, particleHistory, globalBest = eval_and_update ( particleFuture, delayedEvalParticles, particleHistory, paramRanges, globalBest, mode['randomSearch'], nEvaluations )\n",
    "\n",
    "                # append future work for the next instantiation of this particle ( using the freshly updated parameters )\n",
    "                delayedEvalParticles.append( delayed ( evaluate_particle )( newParticle, dataFutures, earlyStoppingRounds, retainPredictionsFlag ))\n",
    "                \n",
    "                nEvaluations += 1\n",
    "            # --- \n",
    "            print(f' > on epoch {iEpoch} out of {nEpochs}') \n",
    "    \n",
    "    else:\n",
    "        # ----------------------------\n",
    "        # asynchronous particle swarm\n",
    "        # ----------------------------\n",
    "        delayedEvalParticles, initialParticleParams, globalBest, particleColors = initialize_particle_futures ( nParticles, paramRanges, randomSeed, plotFlag )\n",
    "        futureEvalParticles = client.compute( delayedEvalParticles )        \n",
    "        particleFutureSeq = as_completed( futureEvalParticles )\n",
    "        \n",
    "        for particleFuture in particleFutureSeq:\n",
    "            newParticle, particleHistory, globalBest = eval_and_update ( particleFuture, delayedEvalParticles, particleHistory, paramRanges, globalBest, mode['randomSearch'], nEvaluations )\n",
    "            \n",
    "            # termination conditions \n",
    "            if globalBest['accuracy'] > terminationAccuracy: break\n",
    "            approximateEpoch = nEvaluations // nParticles\n",
    "            if ( approximateEpoch ) > nEpochs : break\n",
    "            \n",
    "            # append future work for the next instantiation of this particle ( using the freshly updated parameters )\n",
    "            delayedParticle = delayed ( evaluate_particle )( newParticle, dataFutures, earlyStoppingRounds, retainPredictionsFlag )\n",
    "            # submit this particle future to the client ( returns a future )\n",
    "            futureParticle = client.compute( delayedParticle )\n",
    "            # track its completion via the as_completed iterator \n",
    "            particleFutureSeq.add( futureParticle )\n",
    "            \n",
    "            nEvaluations += 1\n",
    "            if nEvaluations % nParticles == 0:\n",
    "                print(f' > on approximate epoch {approximateEpoch} out of {nEpochs}') \n",
    "                              \n",
    "    elapsedTime = time.time() - startTime\n",
    "    \n",
    "    print(f\"\\n\\n best accuracy: {globalBest['accuracy']}, by particle: {globalBest['particleID']} on eval: {globalBest['iEvaluation']} \")\n",
    "    print(f\" best parameters: {swarm.format_params( globalBest['params'], globalBest['nTrees'] )}, \\n elpased time: {elapsedTime:.2f} seconds\")\n",
    "    \n",
    "    particleHistory['initialParams'] = initialParticleParams\n",
    "    particleHistory['paramRanges'] = paramRanges\n",
    "    particleHistory['particleColors'] = particleColors\n",
    "    particleHistory['nParticles'] = nParticles\n",
    "    return particleHistory, globalBest, elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect/Edit Library Code [optional]\n",
    "> swarm.import_library_function_in_new_cell ( **function-name** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleImportFlag = False\n",
    "if sampleImportFlag:\n",
    "    swarm.import_library_function_in_new_cell ( [ swarm.sample_params, swarm.initialize_particle_swarm] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'compute-cluster'></a>\n",
    "\n",
    "# 4. Create Compute Cluster\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dask.distributed import worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster( ip = '', n_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # TODO -- finalize dask_kubernetes check on RAPIDS 0.10.0 cuda 10.1\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster( ip = '' )\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client( cluster, asynchronous = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'define-search'></a>\n",
    "\n",
    "# 5. Define Search: \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper-parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 3, 20, 'int'],\n",
    "                1: ['learning_rate', .001, 1, 'float'],\n",
    "                2: ['gamma', 0, 2, 'float'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SWARM size (nParticles) particles and search duration (nEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nParticles = 16\n",
    "nEpochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Baseline / Demo : Single Particle for a Single Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = {'allowAsyncUpdates': False, 'randomSearch': False }\n",
    "particleHistory, globalBest, _ = run_hpo ( client, mode, paramRanges, \n",
    "                                           trainData, trainLabels, testData, testLabels, \n",
    "                                           nParticles = 1, nEpochs = 1, randomSeed = 0, retainPredictionsFlag = False,\n",
    "                                           earlyStoppingRounds = 15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'run-async-PSO'></a>\n",
    "# 6. Run **ASYNC-Particle-Swarm** \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance: Typically **better performance** compared to synchronous-particle-swarm and random-search.\n",
    "\n",
    "Wall Clock Time: Typically this is the **fastest** search process.\n",
    "\n",
    "* Description: Particles move using a random mixture of personal and global best [ possibly stale ]\n",
    "\n",
    "* Parallelism: Asynchronous\n",
    "  * NOTE: Particles update without waiting for their peers causing faster evaluating hyper-parameter particles to run more often ( see nEvaluations figure )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = {'allowAsyncUpdates': True, 'randomSearch': False }\n",
    "particleHistoryAsync, globalBestAsync, _ = run_hpo ( client, mode, paramRanges,\n",
    "                                                     trainData, trainLabels, testData, testLabels, \n",
    "                                                     nParticles, nEpochs,\n",
    "                                                     wMomentum = .05, wIndividual = .25, wBest = .45 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedBarHeightsDF = swarm.plot_eval_distribution ( particleHistoryAsync,  globalBestAsync['particleID'] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.viz_particle_movement( particleHistoryAsync )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'run-classic-PSO'></a>\n",
    "# 7. Run **Classic-Particle-Swarm**\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Typically classic-particle-swarm reaches higher performance than random-search, but lower performance relative to async-particle-search\n",
    "* Wall Clock Time: Typically this is search process.\n",
    "\n",
    "* Performance: Typically this search method reaches the best result.\n",
    "\n",
    "\n",
    "* Description: Particles move using a random mixture of personal and global best\n",
    "\n",
    "* Parallelism: Partial-Asynchronous\n",
    "  * NOTE: Particles update in parallel with their peers within an epoch, but wait for the slowest particle at epoch boundaries. All particles are guaranteed to have the same number of evaluations ( see nEvaluations figure )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = {'allowAsyncUpdates': False, 'randomSearch': False }\n",
    "particleHistory, globalBest, _ = run_hpo ( client, mode, paramRanges, \n",
    "                                           trainData, trainLabels, testData, testLabels,\n",
    "                                           nParticles, nEpochs,\n",
    "                                           wMomentum = .05, wIndividual = .25, wBest = .45 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedBarHeightsDF = swarm.plot_eval_distribution ( particleHistory,  globalBest['particleID'] );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.viz_particle_movement( particleHistory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'run-random-search'></a>\n",
    "# 8. Run **Random-Search-Baseline**\n",
    "-----\n",
    "\n",
    "Performance: Typically random search **performs worse** than the particle swarm variants.\n",
    "\n",
    "Wall Clock Time: Typically this is the **slowest** search process.\n",
    "\n",
    "* Description: Particles telport to new [random ] positions in hyper-parameter space\n",
    "* Parallelism: Asynchronous\n",
    "  * NOTE: Particles update without waiting for their peers, and randomly adopt new parameter combinations. Long runs (e.g.,  nEpochs = 30000) should produce balanced nEvaluations per-particle, though imbalance is often visible for shorter runs [ less imbalanced than ASYNC PSO, more imbalanced than PSO ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = {'allowAsyncUpdates': True, 'randomSearch': True }\n",
    "particleHistoryBaseline, globalBestBaseline, _ = run_hpo ( client, mode, paramRanges, \n",
    "                                                           trainData, trainLabels, testData, testLabels,  \n",
    "                                                           nParticles, nEpochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedBarHeightsDF = swarm.plot_eval_distribution ( particleHistoryBaseline,  globalBestBaseline['particleID'] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.viz_particle_movement( particleHistoryBaseline )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'summary'></a>\n",
    "# 9. Summary\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async Scaling > Sync Scaling > Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "larger than single GPU memory datasets - dask_cudf + [ dask_xgboost or xgboost.dask ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
