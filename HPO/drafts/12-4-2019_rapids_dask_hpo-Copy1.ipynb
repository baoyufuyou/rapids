{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Scaling XGBoost Hyper-Parameter Optimization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/swarm.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach highest performance in classification tasks (i.e., supervised learning ), it is best practice to build an ensemble of champion models. \n",
    "\n",
    "Each member of the ensemble is a winner of a search over many models of its kind with altered hyper-parameters.\n",
    "\n",
    "In this notebook, we build a harness for running such a [hyper-parameter] search to demonstrate the accuracy benefits while exploring performance as we scale within and accross GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np; import pandas as pd; import cudf\n",
    "import cuml; import xgboost; from xgboost import plot_tree\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import time; import copy \n",
    "\n",
    "import data_utils\n",
    "import swarm\n",
    "import visualization as viz\n",
    "\n",
    "# reload library modules/code without a kernel restart\n",
    "import importlib; importlib.reload( swarm ); importlib.reload( data_utils ); importlib.reload( viz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> In this notebook you can try different hyper-parameter search methods using synthetic or real data. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/datasets.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion / Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dataset = data_utils.Dataset( 'fashion-mnist')\n",
    "# dataset = data_utils.Dataset( 'airline', nSamples = 1000000)\n",
    "# dataset = data_utils.Dataset( 'synthetic', coilType = 'helix', coilDensity = 9, nSamples = 5000000)\n",
    "dataset = data_utils.Dataset( 'synthetic', coilType = 'whirl', coilDensity = 18, nSamples = 5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "**Objective**: As is typical in machine learning workflows we need to split the dataset into a **train-set** and **test-set**. The test-set is unseen during training and model performance on the test-set is an indicator of how well our model(s) can generalize to future data [ also unseen during training ].\n",
    "\n",
    "\n",
    "> **Note**: In the case of synthetic data, we use a non-standard way of splitting the dataset in order to increase the difficulty of the problem and make it a challenge worthy of HPO. Specifically, we carve out the middle of the dataset along one of the synthetic dimensions [ x ], and assign these points to the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentTrain = .75\n",
    "if dataset.datasetName != 'synthetic':    \n",
    "    trainData, testData, trainLabels, testLabels = cuml.train_test_split( dataset.data, dataset.labels, train_size = .75 )\n",
    "    \n",
    "else:    \n",
    "    # samples to exchange between train and test set [ enables generalization in the synthetic data case ]\n",
    "    samplesToSwap = int( dataset.data.shape[0] * .002 ) \n",
    "    # in the case of the helix increase the amount of training data since its a \n",
    "    if dataset.coilType == 'helix': percentTrain = .885 \n",
    "    trainData, testData, trainLabels, testLabels = data_utils.split_synthetic ( dataset.data, dataset.labels, \n",
    "                                                                                percentTrain = percentTrain, \n",
    "                                                                                samplesToSwap = samplesToSwap )\n",
    "\n",
    "dataset.assign_dataset_splits ( trainData, testData, trainLabels, testLabels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Apply standard scaling to each feature column of the dataset to ensure the data is numerically centered and ready for ingestion by an upstream model.\n",
    "**Note**: This is an inplace operation which changes the value of the argument dataframe passed in without returning a value.\n",
    "\n",
    ">More formally this transformation attempts to create normally distributed data features with 0 mean and unit variance.\n",
    "This is accomplished by computing the means and standard deviations of each column in the train-set, and applying these statistics to normalize both the train and test sets.\n",
    "\n",
    "> <font size=\"5\">$x_{rescaled} = \\frac{ x - \\mu_{train} }{\\sigma_{train}}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before scaling\n",
    "data_utils.print_stats( dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standard scaling inplace to train-set and test-set\n",
    "trainMeans, trainSTDevs, _ = data_utils.scale_dataframe_inplace ( dataset.trainData, label='train-set', datasetObject = dataset )\n",
    "_,_,_ = data_utils.scale_dataframe_inplace ( dataset.testData, trainMeans, trainSTDevs, label='test-set', datasetObject = dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after scaling\n",
    "data_utils.print_stats( dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization \n",
    "\n",
    "**Objective**: Build intuition and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the dataset in its raw form [ no re-scaling ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_data( dataset.data, dataset.labels, dataset.datasetName )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Train vs Test [ after split & rescaling ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_train_vs_test(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Hyper-Parameter Choice [ a.k.a., Can you beat HPO? ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexible parameters -- read more @ https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "manualXGBoostParams = { \n",
    "    'max_depth': 6,                     # default = 6             :: maximum depth of a tree\n",
    "    'num_boost_round': 50,              # default = XXX           :: number of trees        \n",
    "    'learning_rate': 0.3,               # default = 0.3           :: step size shrinkage between rounds, prevents overfitting\n",
    "    'gamma': 0.,                        # default = 0             :: minimum loss reduction required to make a leaf node split, prevents overfitting\n",
    "    'lambda': 1.,                       # default = 1             :: L2 regularizaiton term on weights, prevents overfitting\n",
    "    'alpha': 0.,                        # default = 0             :: L1 regularization term on weights, prevents overfitting\n",
    "    'tree_method': 'gpu_hist',          # default = 'gpu_hist'    :: tree construction algorithm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note that we'll inherit objective function from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.evaluate_manual_params(dataset, manualXGBoostParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dask.distributed import worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster( ip = '', n_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client( cluster, asynchronous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define HPO XGBoost Search Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramRanges = { 0: ['max_depth', 2, 15, 'int'],\n",
    "                1: ['learning_rate', .001, 2, 'float'],\n",
    "                2: ['gamma', 0., 3., 'float'] }\n",
    "# number of trees will be allowed to go from 0-1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "| method name | &nbsp;&nbsp;&nbsp; performance | &nbsp;&nbsp;&nbsp; search duration  |\n",
    "|-----------------------|-----------------|------------------|\n",
    "| random-search         | &nbsp;&nbsp;&nbsp; worst | &nbsp;&nbsp;&nbsp; slow    |\n",
    "| particle-search [1]      | &nbsp;&nbsp;&nbsp; good  | &nbsp;&nbsp;&nbsp; fast    |\n",
    "| async-particle-search | &nbsp;&nbsp;&nbsp; best  | &nbsp;&nbsp;&nbsp; fastest |\n",
    "\n",
    "<center>[1] https://en.wikipedia.org/wiki/Particle_swarm_optimization#Algorithm</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Search Strategy : Particle Swarm, Random Search, Grid Search\n",
    "# Sync vs Async [ Dask Task Stream ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/sync_vs_async.png' width='1000px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Params [ nParticles & nEpochs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Perf [ <- ]\n",
    "You'll have need to have launched the container w Port Open\n",
    "Connect via [ the http:// is important]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color = '#ffb500'> Synchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syncSwarm = swarm.SyncSwarm( client, dataset, paramRanges, nParticles = 10, nEpochs = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syncSwarm.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#ffb500'> Synchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( syncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( syncSwarm, topN = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( syncSwarm, syncSwarm.paramRanges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color='#7400ff'> Asynchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncSwarm = swarm.AsyncSwarm( client, dataset, paramRanges, nParticles = 10, nEpochs = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncSwarm.run_search( syncWarmupFlag = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#7400ff'> Asynchronous Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( asyncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( asyncSwarm, topN = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( asyncSwarm, asyncSwarm.paramRanges )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HPO - <font color='#666666'> Random Search </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAsyncSwarm = swarm.RandomSearchAsync ( client, dataset, paramRanges, nParticles = 8, nEpochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAsyncSwarm.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize <font color='#666666'> Random Swarm </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_particle_evals( randomAsyncSwarm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_particle_trails( randomAsyncSwarm, topN = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_swarm( randomAsyncSwarm, randomAsyncSwarm.paramRanges )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine best swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if syncSwarm.globalBest['accuracy'] > asyncSwarm.globalBest['accuracy']:\n",
    "    swarm = syncSwarm\n",
    "else:\n",
    "    swarm = asyncSwarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost model with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParams = {\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'random_state': 0, \n",
    "    'max_depth': int(swarm.globalBest['params'][0]),\n",
    "    'learning_rate': swarm.globalBest['params'][1],\n",
    "    'gamma': swarm.globalBest['params'][2]\n",
    "}\n",
    "    \n",
    "bestParams['objective'] = dataset.trainObjective[0]\n",
    "if dataset.trainObjective[1] is not None: \n",
    "    bestParams['num_class'] = dataset.trainObjective[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainDMatrix = xgboost.DMatrix( data = dataset.trainData, label = dataset.trainLabels )\n",
    "testDMatrix = xgboost.DMatrix( data = dataset.testData, label = dataset.testLabels )\n",
    "trainedModelGPU = xgboost.train( dtrain = trainDMatrix, evals = [(testDMatrix, 'test')], params = bestParams,\n",
    "                                 num_boost_round = swarm.globalBest['nTrees'], verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "trainedModelGPU.save_model('xgb.model.hpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import ForestInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ForestInference.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.trainOb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = ForestInference.load( filename='xgb.model.hpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = ForestInference.load( filename='xgb.model.hpo',\n",
    "                           algo='BATCH_TREE_REORG',\n",
    "                           output_class=True,\n",
    "                           threshold=0,\n",
    "                           model_type='xgboost' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( dataset.testData )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.testData.as_gpu_matrix(order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iColumn in dataset.testData.columns:    \n",
    "    dataset.testData[iColumn] = dataset.testData[iColumn].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.testData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainedModelGPU.predict( testDMatrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filPredictions = fm.predict ( dataset.testData, \n",
    "                              algo='BATCH_TREE_REORG', \n",
    "                              output_class=True, \n",
    "                              threshold=0, \n",
    "                              model_type='xgboost' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudf.Series(filPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform prediction on the model loaded from path\n",
    "fil_preds = fm.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = trainedModelGPU.eval(testDMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataPerf = 1 - float( predictions.split(':')[1] )\n",
    "print(f'accuracy: {testDataPerf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Up Results [ DGX-2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/synthetic_async.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async Scaling > Sync Scaling > Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.osgeo.cn/matplotlib/gallery/lines_bars_and_markers/timeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work / Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "larger than single GPU memory datasets - dask_cudf + [ dask_xgboost or xgboost.dask ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ Generate a classification dataset on GPU ](#data-load) (e.g., double helix, unwinding helix/whirl )\n",
    "\n",
    "2. [ ETL - process/prepare data for model training ](#ETL) (e.g., scale, split, augment )   \n",
    "    \n",
    "3. [ Define HPO Strategy ](#define-hpo)\n",
    "\n",
    "4. [ Create Compute Cluster ](#compute-cluster)\n",
    "   > LocalCUDACluster or KubeCluster\n",
    "      \n",
    "5. [ Define Seach ](#define-search)\n",
    "\n",
    "6. [ Run ASYNC Particle Swarm ](#run-async-PSO)\n",
    "\n",
    "7. [ Run Classic Particle Swarm ](#run-classic-PSO)\n",
    "\n",
    "8. [ Run Random Search Baseline ](#run-random-search)\n",
    "\n",
    "9. [ Summary ](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Choices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is able to make several key choices in running this notebook. They are as follows:\n",
    "\n",
    "1. [ Dataset ]()\n",
    "2. [ Compute Scaling Strategy - Scale-Up, Scale-Out ]()\n",
    "3. [ XGBoost Parameter Search Range ]()\n",
    "\n",
    "4. [ Particle Swarm Type ]()\n",
    "   * Synchronous\n",
    "   * Asynchronous\n",
    "   * Random Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
