{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img width='80%' src='rapids_workflow.png'></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>RAPIDS Demo - End-To-End ML Workflow - Dask Early Draft </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, sklearn\n",
    "from sklearn import datasets\n",
    "import cudf, numba, scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array, dask.dataframe\n",
    "from dask import delayed\n",
    "import dask_cudf\n",
    "from dask.diagnostics import ResourceProfiler #Profiler, CacheProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 2. Generate Dataset [ X: features, y: labels ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the size of the generated dataset -- the number of total samples is determined by this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTotalSamples = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use <a href='https://scikit-learn.org/stable/datasets/index.html#generated-datasets'>sklearn.datasets</a> to build synthetic sub-datasets of the size we specified above. We'll build three sub-datasets and combine them together and then use a trained model to see if we can determine which of sub-dataset a sample belongs to. The three sub-datasets are built using the moons, blobs, and swiss-roll generators. These sub-datasets were selected for their distinct visual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask [ sharded data gen & cudf conversion ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nShards = 4\n",
    "\n",
    "nDatasets = 3\n",
    "\n",
    "nSamplesPerShardSubset = (nTotalSamples//nShards)//nDatasets\n",
    "nSamplesPerShard = nSamplesPerShardSubset*nDatasets\n",
    "nDatasetCols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamplesPerShard, nSamplesPerShardSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_shard ( nSamplesPerShardSubset ):\n",
    "    \n",
    "    # generate features\n",
    "    swissRollDataset = datasets.make_swiss_roll( n_samples = nSamplesPerShardSubset, noise = .005)[0]\n",
    "\n",
    "    moonsDataset = datasets.make_moons(n_samples = nSamplesPerShardSubset, noise = 0)[0]\n",
    "    moonsDataset = np.hstack( [moonsDataset, np.zeros( (moonsDataset.shape[0], 1) )] )*5\n",
    "\n",
    "    blobsDataset = datasets.make_blobs( n_samples = nSamplesPerShardSubset, centers = 5,  n_features = 3, \n",
    "                                        cluster_std = 0.25,  random_state = 0)[0] + [0, 1.5, 0]\n",
    "    \n",
    "    # generate labels for classification \n",
    "    blobsLabels = np.zeros(blobsDataset.shape[0])\n",
    "    moonsLabels = 1 * np.ones(moonsDataset.shape[0])\n",
    "    sRollLabels = 2 * np.ones(swissRollDataset.shape[0])\n",
    "\n",
    "    features = np.vstack([blobsDataset, moonsDataset, swissRollDataset])\n",
    "    labels = np.hstack( [blobsLabels, moonsLabels, sRollLabels] )\n",
    "    \n",
    "    return np.asfortranarray(np.hstack( [ features, np.expand_dims(labels,axis=1) ] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dask Compute Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayedDataShards = [ delayed(generate_data_shard)(nSamplesPerShardSubset) for iShard in range(nShards) ]\n",
    "delayedDataFrames = [ delayed(pd.DataFrame)(iShard) for iShard in delayedDataShards ]\n",
    "delayedGPUDataFrames = [ delayed(cudf.from_pandas)(iShard) for iShard in delayedDataFrames]\n",
    "mergedGPUDataFame = delayed(cudf.concat)(delayedGPUDataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedGPUDataFame.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "with ResourceProfiler(dt=0.25) as rprof:\n",
    "    out = mergedGPUDataFame.compute()\n",
    "print( time.time() - tic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprof.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Dask Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nSamplesPerSubDataset = nTotalSamples//3\n",
    "\n",
    "swissRollDataset = datasets.make_swiss_roll( n_samples = nSamplesPerSubDataset, noise = .005)[0]\n",
    "\n",
    "moonsDataset = datasets.make_moons(n_samples = nSamplesPerSubDataset, noise = 0)[0]\n",
    "moonsDataset = np.hstack( [moonsDataset, np.zeros( (moonsDataset.shape[0], 1) )] )*5\n",
    "\n",
    "blobsDataset = datasets.make_blobs( n_samples = nSamplesPerSubDataset, centers = 5,  n_features = 3, \n",
    "                                    cluster_std = 0.25,  random_state = 0)[0] + [0, 1.5, 0]\n",
    "\n",
    "X = np.vstack([blobsDataset, moonsDataset, swissRollDataset])\n",
    "\n",
    "# generate labels for classification \n",
    "blobsLabels = np.zeros(blobsDataset.shape[0])\n",
    "moonsLabels = 1 * np.ones(moonsDataset.shape[0])\n",
    "sRollLabels = 2 * np.ones(swissRollDataset.shape[0])\n",
    "\n",
    "y = np.hstack( [blobsLabels, moonsLabels, sRollLabels] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Split Train (75%) and Test (25%) Data \n",
    "We split our combined dataset into two portions:\n",
    "* **train-set** - which we'll use to optimize our model's parameters [ train-set = randomly selected 75% of total data]\n",
    "* **test-set** - which we'll use to evaluate how well our trained model performs on unseen data [ test-set = remaining 25% of data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25, random_state = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Visualize Data\n",
    "We define a function for WebGL based 3D plotting using the <a href='https://github.com/maartenbreddels/ipyvolume'>ipyvolume</a> library -- we restrict the maximum points to plot to `maxSamplesToPlot` which has a default setting of `100000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipv_plot_data( data, colorStack = 'purple', \n",
    "                  maxSamplesToPlot = 100000, \n",
    "                  holdOnFlag = False, markerSize=.5):\n",
    "    \n",
    "    nSamplesToPlot = np.min( ( len(data), maxSamplesToPlot ) )\n",
    "    if not holdOnFlag: ipv.figure(width=600,height=600)\n",
    "        \n",
    "    if isinstance(colorStack, np.ndarray):\n",
    "        colorStack = colorStack[0:maxSamplesToPlot,:]\n",
    "\n",
    "    ipv.scatter( data[0:nSamplesToPlot,0], \n",
    "                 data[0:nSamplesToPlot,1], \n",
    "                 data[0:nSamplesToPlot,2], size = markerSize, \n",
    "                 marker = 'sphere', color = colorStack)\n",
    "    \n",
    "    if not holdOnFlag: ipv.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Datasets [ moons, blobs, swiss-roll ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv_plot_data( moonsDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv_plot_data( blobsDataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv_plot_data( swissRollDataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Dataset Plot\n",
    "The train-set is shown in purple and the test-set is show in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "ipv.figure()\n",
    "ipv_plot_data( X_train, 'purple', 200000, True)\n",
    "ipv_plot_data( X_test, 'yellow', 1000, True, 1)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# 3. ETL \n",
    "First we write the dataset to disk (as a comma separated file - CSV) so that we can subsequently demonstrate data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.DataFrame(data = X_train).to_csv('X_train.csv.txt', index = False)\n",
    "pd.DataFrame(data = X_test).to_csv('X_test.csv.txt', index = False)\n",
    "pd.DataFrame(data = y_train).to_csv('y_train.csv.txt', index = False)\n",
    "pd.DataFrame(data = y_test).to_csv('y_test.csv.txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'no data\\n0' > warmup.csv # write a mini csv file used to initialize cuIO kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Size of Data on Disk \n",
    "using the default value of `nTotalSamples = 5000000` should produce a training set of `~184MBs` in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h *csv.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "startTime = time.time()\n",
    "\n",
    "pd_X_train = pd.read_csv('X_train.csv.txt',  delimiter=',')\n",
    "pd_X_test = pd.read_csv('X_test.csv.txt',  delimiter=',')\n",
    "pd_y_train = pd.read_csv('y_train.csv.txt',  delimiter=',')\n",
    "pd_y_test = pd.read_csv('y_test.csv.txt',  delimiter=',')\n",
    "\n",
    "pandasIngestionTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column-names\n",
    "f = open('X_train.csv.txt'); colNames = f.readline().strip().split(','); f.close()\n",
    "# warmup rapids data ingestion engines [ cuio kernels ]\n",
    "cudf.read_csv('warmup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "cudf_X_train = cudf.read_csv('X_train.csv.txt', delimiter=',', skiprows=1, names=colNames, dtype=['float64', 'float64', 'float64'])\n",
    "cudf_X_test = cudf.read_csv('X_test.csv.txt', delimiter=',', skiprows=1, names=colNames, dtype=['float64', 'float64', 'float64'])\n",
    "cudf_y_train = cudf.read_csv('y_train.csv.txt', dtype=['float64'])\n",
    "cudf_y_test = cudf.read_csv('y_test.csv.txt', dtype=['float64'])\n",
    "\n",
    "rapidsIngestionTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Load/Ingestion Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasIngestionTime/rapidsIngestionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 - Transform Data ( Normalize )\n",
    "Transforming a dataset is a common requirement prior to training upstream models. For each feature in the dataset we remove the mean and divide by the standard deviation -- this makes each feature behave like a normally distributed variable (e.g. gaussian with 0 mean and unit variance). \n",
    "\n",
    "For the data on the CPU we can use the pre-built <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html'>sklearn.preprocessing.StandardScaler</a> function.\n",
    "In the case of the GPU, we demonstrate how the same transformation can be built using a custom (user defined) function written as a <a href='http://numba.pydata.org/numba-doc/0.13/CUDAJit.html'>just-in-time numba kernel</a>. \n",
    "\n",
    "Note that we compute the mean and standard deviation statistics on the training data, and then apply the transformation to the training and test data (i.e., the test data is never seen when computing the mean & standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "startTime = time.time()\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(pd_X_train) # normalize\n",
    "pd_X_train = scaler.transform(pd_X_train)\n",
    "pd_X_test = scaler.transform(pd_X_test)\n",
    "\n",
    "pandasTransformTime = time.time() - startTime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit \n",
    "def gpu_scale(outputCol, colGPUArrays, colMeans, colStDevs):\n",
    "    iRow = cuda.grid(1)\n",
    "    if iRow < colGPUArrays.size:\n",
    "        outputCol[iRow] = ( colGPUArrays[iRow] - colMeans ) / ( colStDevs + 1e-10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler_numba( targetDF, trainMeans = None, trainStdevs = None):\n",
    "    nRows = targetDF.shape[0]\n",
    "    \n",
    "    blockSize = 128\n",
    "    blockCount = nRows // blockSize + 1\n",
    "    scaledDF = cudf.DataFrame()\n",
    "    \n",
    "    if trainMeans is None and trainStdevs is None:\n",
    "        trainMeans = {}\n",
    "        trainStdevs = {}\n",
    "        \n",
    "    for iColName in targetDF.columns:\n",
    "        colGPUArray = targetDF[iColName].to_gpu_array()\n",
    "        outputCol = cuda.device_array ( shape=(nRows), dtype=colGPUArray.dtype.name)       \n",
    "        if iColName not in trainMeans.keys():\n",
    "            trainMeans[iColName] = targetDF[iColName].mean()\n",
    "        if iColName not in trainStdevs.keys():\n",
    "            trainStdevs[iColName] = targetDF[iColName].std()\n",
    "        gpu_scale[(blockCount),(blockSize)](outputCol, colGPUArray, trainMeans[iColName], trainStdevs[iColName])\n",
    "        scaledDF.add_column(name=iColName, data = outputCol)    \n",
    "        \n",
    "    return scaledDF, trainMeans, trainStdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = standard_scaler_numba( cudf_X_test.copy().head(2) ) # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "cudf_X_train, trainMeans, trainStdevs = standard_scaler_numba( cudf_X_train )\n",
    "cudf_X_test, _, _ = standard_scaler_numba( cudf_X_test, trainMeans, trainStdevs )\n",
    "\n",
    "rapidsTransformTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Transform Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasTransformTime/rapidsTransformTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify [approximate] numerical equivalence between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMeans, scaler.mean_, trainStdevs, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_X_test[0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cudf_X_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 4. - Model Building with XGBoost\n",
    "-----\n",
    "XGBoost is a popular algorithm for classification. It uses a sequence of decision trees built in succession such that each new tree attempts to correct the errors made by its predecessors (analogy to multiple golf swings [ each improving on the past ] to reach a target). For a deeper dive into how XGBoost works check out the following dev blog: <br>\n",
    "> https://devblogs.nvidia.com/gradient-boosting-decision-trees-xgboost-cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='xgboost.png' width =90%>\n",
    "<center> img src: https://explained.ai/gradient-boosting/L2-loss.html </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd_X_train\n",
    "y = pd_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpu = xgboost.DMatrix(pd_X_train, label=np.squeeze(pd_y_train))\n",
    "y_gpu = xgboost.DMatrix(pd_X_test, label=np.squeeze(pd_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noteable parameters: [ to see all available options execute '?xgboost.XGBClassifier' in a new cell] \n",
    "\n",
    "* __max_depth__ : int [ default = 3 ] -- Maximum tree depth for base learners.\n",
    "* __n_estimators__ : int [ default = 100 ] -- Number of boosted trees to fit.\n",
    "* __n_jobs__ : int [ default = 1 ] -- Number of parallel threads used to run xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuMaxDepth = 3 # default\n",
    "nTrees = 100 # default\n",
    "\n",
    "gpuMaxDepth = 10\n",
    "\n",
    "nCores = !nproc --all\n",
    "nCores = int(nCores[0])\n",
    "\n",
    "paramsGPU = {\n",
    "    'max_depth': gpuMaxDepth,\n",
    "    'n_estimators': nTrees,\n",
    "    'n_gpus': 1,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'objective': 'gpu:reg:linear', \n",
    "    'random_state': 0,\n",
    "    'verbose_model': True\n",
    "}\n",
    "paramsCPU = {\n",
    "    'max_depth': cpuMaxDepth,\n",
    "    'n_estimators': nTrees,\n",
    "    'tree_method': 'hist',\n",
    "    'objective': 'binary:logistic',\n",
    "    'n_jobs': nCores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 4.1 - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on **CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostModelCPU = xgboost.XGBClassifier(max_depth = paramsCPU['max_depth'], \n",
    "                                        n_estimators = paramsCPU['n_estimators'],\n",
    "                                        tree_method = paramsCPU['tree_method'],\n",
    "                                        objective = paramsCPU['objective'],                                        \n",
    "                                        n_jobs = paramsCPU['n_jobs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostModelCPU, print(\"using {} CPU cores for parallel xgboost training\".format(nCores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "xgBoostModelCPU.fit( X, np.squeeze(y) );\n",
    "\n",
    "cpuXGBoostTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on **GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "xgBoostModelGPU = xgboost.train( dtrain = X_gpu, params = paramsGPU)\n",
    "\n",
    "gpuXGBoostTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuXGBoostTime/gpuXGBoostTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 4.2 - Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer/predict using Trained **CPU** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "yPredTrain = xgBoostModelCPU.predict(pd_X_train)\n",
    "yPredTest = xgBoostModelCPU.predict(pd_X_test)\n",
    "\n",
    "cpuXGBoostInferenceTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer/predict using Trained **GPU** Model\n",
    "> note that our objective was changed to a regression [ gpu accelerated ] so we must take care to convert each of our predictions from a continuous value to a discrete class (essentially by rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert continuous prediction to a multi-class option\n",
    "def continuous_to_discrete( data, nClasses = 3):\n",
    "    data[data>nClasses-1] = nClasses-1 # filter values beyond the possible classes\n",
    "    return np.abs(np.round(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "yPredTrain_GPU = continuous_to_discrete ( xgBoostModelGPU.predict(X_gpu) )\n",
    "yPredTest_GPU = continuous_to_discrete ( xgBoostModelGPU.predict(y_gpu) )\n",
    "\n",
    "gpuXGBoostInferenceTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Inference Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuXGBoostInferenceTime/gpuXGBoostInferenceTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 4.3 - Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( 'CPU test accuracy: {0:.6f} '.format( accuracy_score(pd_y_test, yPredTest) ))\n",
    "print( 'GPU test accuracy: {0:.6f} '.format( accuracy_score(pd_y_test, yPredTest_GPU) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: to increase model accuracy, increase complexity, number of trees, max_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n confusion matrix on TRAIN data -- ')\n",
    "print(confusion_matrix(pd_y_train, yPredTrain))\n",
    "print('\\n confusion matrix on TEST data -- ')\n",
    "print( confusion_matrix(pd_y_test, yPredTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n confusion matrix on TRAIN data -- ')\n",
    "print(confusion_matrix(pd_y_train, yPredTrain_GPU))\n",
    "print('\\n confusion matrix on TEST data -- ')\n",
    "print( confusion_matrix(pd_y_test, yPredTest_GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 - Visualize Model Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a CPU boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,50))\n",
    "plot_tree(xgBoostModelCPU, num_trees=0, ax=plt.subplot(1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a GPU Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,50))\n",
    "plot_tree(xgBoostModelGPU, num_trees=0, ax=plt.subplot(1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Class Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_colors_to_clusters_topK ( dataset, labels, topK=None, cmapName = 'tab10'):\n",
    "    if topK == None:\n",
    "        topK = dataset.shape[0]\n",
    "    \n",
    "    colorStack = np.zeros((topK, 3), dtype=np.float32)\n",
    "    \n",
    "    cMap = plt.get_cmap(cmapName)\n",
    "    for iColor in range ( topK ):\n",
    "        colorStack[iColor] = cMap.colors[ labels[iColor] ]\n",
    "        \n",
    "    return colorStack    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorStackClassifier = map_colors_to_clusters_topK ( pd_X_test, yPredTest_GPU.astype(np.int), topK=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv_plot_data( pd_X_test, colorStack= colorStackClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Extensions\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ext.1 : Growing the Model Ensemble ( DBScan Clustering )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='95%' src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan via sklearn [ CPU ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-sample\n",
    "nSamplesToCluster = 30000\n",
    "pd_X_test_sampled = pd_X_test[:nSamplesToCluster, :]  \n",
    "cudf_X_test_sampled = cudf_X_test.loc[0:nSamplesToCluster,list(cudf_X_test.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "dbScanModel = DBSCAN( min_samples = 10, n_jobs = nCores ).fit(pd_X_test_sampled)\n",
    "labels = dbScanModel.labels_\n",
    "\n",
    "sklearnDBScanTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan via cuml [ GPU ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startTime = time.time()\n",
    "\n",
    "clustering_cuml = cuml.DBSCAN( eps = .15 , min_samples = 200 )\n",
    "clustering_cuml.fit( cudf_X_test_sampled )\n",
    "\n",
    "rapidsDBScanTime = time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print( clustering_cuml.labels_.value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Clustering Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearnDBScanTime/rapidsDBScanTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Clusters to Colors and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "colorStack = map_colors_to_clusters_topK( pd_X_test, clustering_cuml.labels_, nSamplesToCluster  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colored_topK ( dataset, topK = None):    \n",
    "    if topK == None:\n",
    "        topK == dataset.shape[0]\n",
    "        \n",
    "    return lambda dataset, colorStack, topK : ipv.quickscatter( dataset[0:topK, 0], \n",
    "                                                                dataset[0:topK, 1], \n",
    "                                                                dataset[0:topK, 2], \n",
    "                                                                size = .5, marker = 'sphere', \n",
    "                                                                color = colorStack)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_topK( pd_X_test_sampled, nSamplesToCluster )(pd_X_test_sampled, colorStack, nSamplesToCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO ] Ext.2 : cross-validation\n",
    "### [ TODO ] Ext.3 : hyper-parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# End [ thanks! ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please provide feedback/suggestions and errata @ https://github.com/miroenev/rapids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
